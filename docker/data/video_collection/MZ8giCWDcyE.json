{
    "id": "MZ8giCWDcyE",
    "title": "Smartphone Cameras vs Reality!",
    "channel": "Marques Brownlee",
    "channel_id": "UCBJycsmduvYEL83R_U4JriQ",
    "subscriber_count": 19600000,
    "upload_date": "2021-12-09T01:04:19Z",
    "video_url": "https://www.youtube.com/watch?v=MZ8giCWDcyE",
    "category": "Science & Technology",
    "tags": [
        "smartphone cameras",
        "best camera",
        "best smartphone camera",
        "iPhone camera",
        "Pixel camera",
        "iPhone 13 camera",
        "iPhone 14 camera",
        "Pixel 6 camera",
        "Pixel 7 camera",
        "Xiaomi camera",
        "Huawei camera",
        "MKBHD",
        "camera vs reality",
        "Smartphone camera vs reality"
    ],
    "views": 2724705,
    "likes": 131049,
    "comments_count": 5589,
    "description": "Thoughts on computational photography bending the definition of a \"photo\" Sponsored by Cash App: Download from App Store/Google Play store - Use code MARQUES for $15 and $10 goes to Girls Who Code!  MKBHD Merch:   Tech I'm using right now:   Playlist of MKBHD Intro music:   ~",
    "description_links": [
        "http://shop.MKBHD.com",
        "https://www.amazon.com/shop/MKBHD",
        "https://goo.gl/B3AWV5",
        "http://twitter.com/MKBHD",
        "http://instagram.com/MKBHD",
        "http://facebook.com/MKBHD"
    ],
    "transcript": "- So there are three cameras on the back of the iPhone, 13 Pro. The main camera, the ultra wide and the telephoto. So this one down here at the bottom, this is the main camera. So if you cover it with your finger, you can see the frame goes dark, makes sense. If you cover the other two, nothing happens. Now when you hit that 3X button to zoom into 3X it's supposed to switch to that telephoto camera, right? But sometimes when you zoom into 3X and then cover the main camera, it still goes dark. Why? This is because the iPhone's camera thinks it knows better than you. And it usually does, basically in certain conditions, especially with lower light, you could get a worse photo out of actually switching to the telephoto camera. Which is a smaller sensor with a smaller aperture that lets in less light and can be more noisy. So sometimes when you hit that 3X, it just crops in on the main, big camera, and doesn't even tell you, and that's actually going to give you a better photo. See smartphone cameras are smart but something I've been thinking a lot about lately is they've gone past smart. They're bending reality. So I'm in the middle of running the blind smartphone camera test over on Instagram right now. If you haven't already gotten in on started voting on those, you should do it. It's a fascinating experiment every time. But a thought I've had is maybe it's not just the brightest photo that's going to win every single time. So I actually think that similar to how in this tech bubble, we underestimate how many people put cases on their phones. At least I do. There's also a bit of an underestimation of how many people want to just be able to take a photo and post it with zero edits. Now that's kind of crazy in the tech world or in the photography world. We want the more neutral photo, the one with more information, and that's the better photo to us. 'Cause then we can go take it and edit it and make it exactly how we want, because we want that control. But to most people, if they can just take their phone and point and shoot, and the photo that comes out of that is perfectly good enough to post with no edits at all. That to them is a great camera. And now you're letting, of course the camera do all of the editing for you, which means you have the least amount of control over the final look. Now the danger of giving up all the control is our photos become a product of someone else's vision technically. And this is where it starts to get crazy because every smartphone company sees things a little differently, right? We already know a Pixel photo looks different from an iPhone photo, which looks different from Huawei phones, which look different from Xiaomi shots. Every picture is the result of an image processing pipeline that is tuned by people. And that is a reflection of their biases and their skills and what they think we want. Which means every photo we take, even if it's of the same thing will be slightly different, just depending on what camera you take it with. Which one is real, which one's the most accurate. Maybe it doesn't matter. In 2019, the Huawei P30 Pro came out. It had a pretty solid set of cameras. It was a flagship phone, of course, so people went out and tested its limits and pretty quickly something sort of fishy came up. So when you went outside at night and pointed the new periscope zoom camera at the moon and zoomed all the way in, the camera would recognize the moon and suggest you turn moon mode on. And people started doing this and posting their results. And everyone's pictures of the moon looked surprisingly similar. Now, maybe I shouldn't be shocked. I mean, we're all taking pictures of the same moon, but have you ever tried to take a picture of the moon with your camera on your phone? It's usually just a blob. It never looks that good. And these all looked really good. Maybe a little too good. And that's what Android Authority concluded with enough samples. They believe that Huawei is using AI to not just recognize that you're taking a picture of the moon, but also to then super impose a stored image of the moon and merge it onto your photo. Now at first glance, that's pretty crazy, but that's also kind of clever because the moon is tidally locked with the earth. Meaning one rotation takes the same amount of time as one orbit. So one face of the moon is always pointing towards earth. So you only see one side of the moon all the time, meaning it's always going to look the same and you only need one stored image of the moon to superimpose over everyone's photos. So maybe that's not so bad. But Huawei denied this, of course. But the seed was definitely planted. And my take, honestly at the time was like, all right, well, you have this AI mode in your camera anyway, and it's already recognizing scenes and adjusting things and changing things to enhance your photos already. Why not add a picture of the moon in there. But it does bring up the question, a totally fair question, which is how far is too far? Like, people already seem to want the most finished version of their photos straight out of camera. And so you're doing edits and enhancements. How far is too far. Xiaomi phones we already know can detect a landscape and make the blue sky bluer, or they'll crank up the green on the green grass. But also some of these phones from Chinese vendors have very different acceptable levels of body and face adjustment. So this Xiaomi Mi 11 Ultra when you open the selfie camera has a beauty filter, but this isn't just facial smoothing. It literally lets you move your hair line, changes the shape of your chin and your nose. It can slender up your face. It changes the size of your lips and your cheeks. It can make your eyes bigger or smaller and you can put makeup on yourself. And it's all just built into the camera out the box and it's totally normal and accepted. It kind of reminds me of when there was a sort of a commercialized version of this when the Galaxy S9 in the United States had a Bixby Vision feature to try on makeup. And then you could swap between different shades of lipstick and blush and eye shadow. And then Bixby would give you a link to buy the actual retail version of that makeup. But really the most powerful adjustments are the ones that happen when you don't even know it. And you didn't even ask for it. They happen in the background, it's the highest level of computational photography. So Google's Pixel 6 is always running the main camera at one shutter speed and the ultra-wide camera at a much faster shutter speed at the same time. So if you take a photo of a moving person, the phone detects the face, realizes if it's blurry, it can take a non blurry copy from the ultra wide camera and merge it onto your subject to keep just the face crisp and clear. All of this happens in the background without you even asking. And there's also already a feature in FaceTime on iPhone's called eye contact that moves your pupils to make it look like you're making eye contact with the camera. Even though you're not, you're looking at the screen below the camera, but it's pretty eerie and slightly creepy. And it works a little too well, but at least you can turn it off. And I could swear this was a feature somewhere. I must've been imagining a keynote, but I can't find it anywhere. So I'm gonna predict that this feature will exist at some point in some phone, probably in something like a pixel first. Imagine you're taking a group selfie shot. There's a bunch of people with you. You hit the shutter button and almost everyone has their face not blinking and smiling, but at different moments, everyone has sort of their ideal face. So the software smartly goes through and merges the best smiling, non blinking face for everyone in the selfie. It doesn't even tell you just does it in the background. We've actually seen versions of this working our way up to this feature. So believe it or not in 2012 a Nokia Lumia phone had a selfie mode where you'd hold for five seconds. And then after the shot, you could scroll between five different faces of selfies to pick which one you like the best. So it sounds crazy, but that's the thought I've been having is this is the direction smartphone cameras are going, which is as computational photography gets better and better and we're merging more and more things in. Eventually these cameras are outputting captures of moments in time that never really happened. So it's easy to see a future where smartphone cameras just recognize all kinds of things like AI mode right now is pretty basic. It'll see a sunset and make the oranges brighter, but maybe it'll start recognizing all types of objects. You're in front of a popular Instagram wall in Santa Monica somewhere. And it notices you take a picture in front of it. It's like, oh, I have a downloaded picture of that in our database. And it just wipes out all the people in the background and makes you perfectly flat on that image without you even asking, that could happen in the future. At that point basically the whole world turns into AI recognizable QR codes, where your AI camera's just being triggered by all sorts of objects and things around you to morph into recognized situations. It is kind of crazy to think about, but while we're in this new reflective mode, shout out to new channel sponsor Cash App. Cash App I'd say is just the right level of futuristic. So it's already a great app for sending and requesting money from your friends or get dinner with somebody and just want to reimburse them, that's easy. But you can also buy stocks or buy Bitcoin with it. So if you haven't already signed up, feel free to use the link below, or my code Marques and 15 bucks will just appear in your account, you're welcome. Also $10 will go to Girls Who Code. But yeah, my take is I don't have a solution for this eerily dystopian smartphone camera future, but I wonder, are you okay with smartphone cameras spitting out finished images that are further and further from reality. They're basically bending the definition of a photo. Let me know what you think. Either way, that's been it. Thanks for watching. Catch you guys in the next one, peace. (soft outro music)",
    "transcript_keywords": [
        "camera",
        "photo",
        "moon",
        "main camera",
        "cameras",
        "smartphone cameras",
        "people",
        "face",
        "smartphone",
        "time",
        "telephoto camera",
        "phone",
        "picture",
        "photos",
        "main",
        "things",
        "Huawei",
        "make",
        "phones",
        "crazy"
    ],
    "transcript_entity_values": [
        "AI",
        "Nokia",
        "three",
        "Xiaomi",
        "one",
        "Huawei",
        "FaceTime",
        "13",
        "five seconds",
        "3X",
        "15 bucks",
        "Santa Monica",
        "Marques",
        "five",
        "the Galaxy S9",
        "first",
        "Bixby Vision",
        "iPhone",
        "zero",
        "Google",
        "2012",
        "Android Authority",
        "Chinese",
        "Bixby",
        "Xiaomi",
        "10",
        "this Xiaomi Mi 11 Ultra",
        "2019",
        "Instagram",
        "two",
        "the United States",
        "Bitcoin",
        "night"
    ],
    "transcript_entity_types": [
        "ORG",
        "ORG",
        "CARDINAL",
        "PERSON",
        "CARDINAL",
        "ORG",
        "PRODUCT",
        "CARDINAL",
        "TIME",
        "CARDINAL",
        "MONEY",
        "GPE",
        "ORG",
        "CARDINAL",
        "ORG",
        "ORDINAL",
        "ORG",
        "ORG",
        "CARDINAL",
        "ORG",
        "DATE",
        "ORG",
        "NORP",
        "PERSON",
        "PRODUCT",
        "MONEY",
        "PERSON",
        "DATE",
        "ORG",
        "CARDINAL",
        "GPE",
        "NORP",
        "TIME"
    ]
}