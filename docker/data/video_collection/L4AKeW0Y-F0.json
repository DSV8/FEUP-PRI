{
    "id": "L4AKeW0Y-F0",
    "title": "This is an SSD?! - PureStorage FlashBlade Tour",
    "channel": "Linus Tech Tips",
    "channel_id": "UCXuqSBlHAE6Xw-yeJA0Tunw",
    "subscriber_count": 15900000,
    "upload_date": "2022-07-28T17:00:11Z",
    "video_url": "https://www.youtube.com/watch?v=L4AKeW0Y-F0",
    "category": "Science & Technology",
    "tags": [
        "PureStorage",
        "Pure Storage",
        "SSD",
        "Datacenter",
        "Tour",
        "Data Center",
        "Flashblade",
        "flash blade",
        "FlashBladeS",
        "FlashBlade//S",
        "Flash Blade S"
    ],
    "views": 1340932,
    "likes": 57384,
    "comments_count": 2155,
    "description": "Thanks to Pure Storage for sponsoring this video! You can learn more about FlashBlade//S at  and follow Pure Storage on LinkedIn at   SSDs are fast, but in the datacenter, fast is never fast enough. So why did Pure Storage switch to slower QLC flash, and how is it FASTER than TLC?  Discuss on the forum:    GET MERCH:   SUPPORT US ON FLOATPLANE:   AFFILIATES, SPONSORS & REFERRALS:   PODCAST GEAR:    FOLLOW US  ---------------------------------------------------   Twitter:  Facebook:  Instagram:  TikTok:  Twitch:   MUSIC CREDIT --------------------------------------------------- Intro: Laszlo - Supernova Video Link:  iTunes Download Link:  Artist Link:   Outro: Approaching Nirvana - Sugar High Video Link:  Listen on Spotify:  Artist Link:   Intro animation by MBarek Abdelwassaa  Monitor And Keyboard by vadimmihalkevich / CC BY 4.0   Mechanical RGB Keyboard by BigBrotherECE / CC BY 4.0  Mouse Gamer free Model By Oscar Creativo / CC BY 4.0   CHAPTERS --------------------------------------------------- 0:00  Intro 0:50  This is an SSD?! 1:59  How do SSDs work? 2:59  The Method to Pure Storages Madness 6:17  The FlashBlade//S 8:12  Touring the Datacenter 11:44  Who could need this much storage!? 13:15  An NVIDIA DGX A100 in action 14:08  What happens after the SSD array is retired? 15:10  The File System and what happens when you yank one out 17:45  Who is Pure Storage? 18:32  Conclusion",
    "description_links": [
        "https://purefla.sh/3u1ySFF",
        "https://purefla.sh/3xMZlIk",
        "https://linustechtips.com/topic/1446048-this-is-an-ssd-sponsored/",
        "https://lttstore.com",
        "https://www.floatplane.com/",
        "https://lmg.gg/sponsors",
        "https://lmg.gg/podcastgear",
        "https://twitter.com/linustech",
        "http://www.facebook.com/LinusTech",
        "https://www.instagram.com/linustech",
        "https://www.tiktok.com/@linustech",
        "https://www.twitch.tv/linustech",
        "https://www.youtube.com/watch?v=PKfxmFU3lWY",
        "https://itunes.apple.com/us/album/supernova/id936805712",
        "https://soundcloud.com/laszlomusic",
        "https://www.youtube.com/watch?v=ngsGBSCDwcI",
        "http://spoti.fi/UxWkUw",
        "http://www.youtube.com/approachingnirvana",
        "https://www.instagram.com/mbarek_abdel/",
        "https://geni.us/PgGWp",
        "https://geni.us/mj6pHk4",
        "https://geni.us/Ps3XfE"
    ],
    "transcript": "- This is an SSD. No, no, seriously. This whole thing is a complete SSD and it's designed for massive data centers Like what you might find at the likes of meta. So how come I've never heard of it? Well, Pure Storage wonders that too And they've sponsored this video for us to take a tour of their facilities here in Sunny Mountain View, California and dig deeper into just how these things work. And maybe while they're at it, they can explain a little bit about how they can make the slowest flash used in SSDs today run twice as fast as last year's good stuff. Let's take a look. (techno music) (robot blipping) (techno upbeat music) From the beginning, Pure Storage's secret sauce has been in their approach to software. While their solution is proprietary, with it, they're able to tightly control each individual NAND flash chip in array potentially the size of a whole rack. But there's no way that I know of to drill down to that level of detail with any normal SSD. Like even enterprise grade SAS and U.2 drives have controllers that make the drives show up as disks. Like you don't get direct access to the chips, so what's up with that? This is a direct flash module, or DFM for short. It's a strange looking module. It's a lot larger than any SSD form factor I've seen before. And there's a lot of NAND flash on there. Up to 48 terabytes in the current generation. Along with super capacitors to make sure writes complete properly in the case of a sudden power loss. Pretty standard for data center SSD, yes, but while you'd expect loads of DRAM cache to make it fast, there's no DRAM on here. That is a terrible thing for performance under normal circumstances, just because of the way SSDs work. Whenever an individual die is busy performing some action, the data on it is completely inaccessible and you have to wait. DRAM caches collect the data and delay writing it until there's enough of it that it actually makes sense to spend the time actually writing it out, because in order for an SSD to write that data, any blocks that aren't totally empty first have to be erased. As you add additional levels of storage to a cell, so from single to multi, to triple, to quad, the time it takes to actually do this increases exponentially. And if you remember from our review of Intel's early DRAM list QLC drives, you would know that it can be slower than a hard drive. This could be catastrophic for real-time applications if your DRAM was array, decided that it was time for some spring cleaning. So have Pure Storage just gone completely mad? If so, well, there may be a method to their madness. Up to four of these DFMs can slot into a blade like this one, which itself is a self-contained server running a socketed Xeon processor. There's the DRAM. Yes, this Xeon CPU is acting as a glorified SSD controller. Now, it does other array management stuff too, but to talk to the storage, it actually runs a very low level interface bootstrap from Linux that talks to a custom controller on each DFM. These controllers do one very simple task, provide an interface to all the flash on the module. It doesn't really do anything that you'd expect from a typical SSD controller. All of that is handled by the CPU. While it does use the NVMe protocol and a U.2 connection, it won't show up as a disk in any other system simply because it doesn't have the brain. This arrangement lets them directly control when, where, and how new data gets soared. That is kinda like Apple Silicon in a way except they can handle multiple vendors of flash in the same array. More importantly, there's no pretending to be a hard drive. So while block sizes can vary between vendors, you're not forced to write out a whole four kilobytes each time you need to write a couple of bytes. So because the blade knows what the whole array looks like, data that's changed very frequently can be grouped together to prevent a situation known as tombstoning, where deleted but not erased data can stick around because uneconomical to evicted, which means that that cell is effectively never touched again. That's partly why overprovision space, which is where a percentage of the raw capacity is reserved for system use is important for SSD performance. A workaround that Pure Storage doesn't really need. In a nutshell, each of these blades is basically an SSD with extra steps. Because of that, Pure Storage is able to control things like when and where to start where leveling, which is incredibly important to QLC flash because it's typically only rated for less than a thousand program erase cycles. So obviously, there's a lot of incentive to never have to cycle a cell if you don't have to. To help with that, data is compressed wherever possible. There's another component to this too, because you can only ever read or write flash memory. Not both. The more you can avoid writing to frequently red cells or overwriting frequently written data to those cells, the less often you're stuck waiting for the SSD to come around and actually speed out the data you need, and that's especially important with QLC because of how long it takes to erase and rewrite data on that type of flash. Remember, at this kind of scale, you're talking databases that are massive enough and accessed often enough that even a delay of tens of milliseconds could be a major problem. (machine humming) And of course, they've thought about that too. What if I told you that in the time it takes for a write cycle to end, the inaccessible bits can be rebuilt from parity data. Yeah, that's not something you can do with the traditional SSD. The controllers that pretend that they're still spinning disks for the PC's sake quickly become the bottleneck. Let's meet a complete system and see it at work. This is a FlashBlade S chassis, Pure Storage's newest product. Each of these can hold 10 blades with four DFMs each, which add up to 48 terabytes per DFM, is a staggering 1.9 petabytes of storage and a single 5U chassis. Remember the lengths we had to go to in order to get a petabyte of hard drives? And then how many boxes a petabyte of flash took up? This is next level. There are four power supplies on this thing that are all modular, each with the capability of drawing 2.4 kilowatts from the wall. And while they don't sell them right now, multi-chassis configs with up to 10 of these things in a rack are in the pipeline. A fully-loaded FlashBlade//S has enough redundancy that it can lose a whole blade and a DFM in another blade and be totally fine. The blades connect to each other via the same back plane that the blades slot into, called fabric I/O modules, or FIOMs for short. Each FlashBlade//S has two FIOMs, which automates managing the blades and chassis units with a total of eight 100 gigabit per second links. Only four of which are currently used. They say that when they're ready to enable the others, they'll be able to do it for free via software update, something they're able to do, thanks to their evergreen model. There's a lot to this, but basically, they don't charge for new software features and their modular approach to hardware means that you can continuously upgrade to the point where one of their alpha stage clients from 10 years ago has continuously upgraded their array with a no downtime to this very day. It's like the array of thesis, none of it is still the same except for the data. And even that might be different now. But enough talk, let's see them roar in the data center. This is a rack full of the previous generation versions of what we were just looking at a little while ago. These are all connected together with a fabric link, this is I think all fibre. And they're all connected via these back planes here. So this top one here is set to go all of the pink cables. And this bottom one is gonna be mostly blue, there's some pink here as well. And all of these are communicating with each other in a maximum layout. So this whole rack is acting as basically a single SSD. As you can see, all of these power connectors are modular. The actual power cables themselves have tabs on them, so you can't just pull them out. So yeah, this is all of their last gen stuff. This over here is a rack full of their new gen stuff. This is FlashBlade//S. What we're looking at here is more or less a setup of a bunch of standalone units. Rather than having everything built together into one giant array, each one of these is a smaller array. Compared to spinning disks, this is actually pretty good power density. Like for the amount of storage you're actually getting here, it's significant savings, like it's something like 1.3 watts per terabyte or something like that. This is one of the first modules that they actually built for this type of array. It actually has an FPGA on it that takes all of its information from an SD card here. And that is a mini-USB port that tells you how long ago this was. But this was NVMe before pretty much anybody was using NVMe. It uses a U.2 connector here to connect directly to the backplane. Basically, I had to create a PCI express backplane for this, because actual controllers, HBAs, didn't really exist at the time for this kind of storage. And I was just handed one of these. This is a tray-less version. It's been taken out of the tray of the DFM that's currently being used. We can see that there are super capacitors on here in order to make sure that in case, say for example, one of these were unceremoniously yanked out the server. Any data that was already being written will be written. So it won't be partially written, it won't get corrupted in that way. Also, on the back, we can see here that we have extra NAND flash chips. This is a 48 terabyte module. 48. 48. That is a massive amount of storage. No hard drive could possibly even come close to this within the next, I don't know how long it would take, because currently, at the rate of growth for hard drives, it would probably be another 10 years or so assuming hard drive is still relevant at that point in time, which is something that Pure Storage is trying to actually avoid. They feel that flash storage is inherently superior to spinning disks. And quite frankly, from what I've seen so far, it definitely kinda looks like it. You get better density, better power draw, and you get a much, much lower level ability to like just manage the data on the drive than you would get from a traditional type of hard drive. Now, you might be asking yourself, who could possibly need this much storage? Well, the answer... Aww, there's rails and stuff here. (host laughs) This is great. So the answer is anybody who's in deep learning. So like, this is an NVIDIA DGX. This is one of the things that Pure Storage has actually partnered with NVIDIA for. Meta recently partnered with Pure Storage and, I think, also NVIDIA for their AI deep learning platform. So the DGX is powering the brains, whereas, Pure Storage is powering all of the data sets that they need to crunch through. So you're never going to get a GPU with petabytes of a video memory, at least, not in the next, I don't know, several decades, at least. So what we're looking at here is an array that's fast enough to keep up with those deep learning workloads on those GPUs to keep them fed, so they can actually be doing their job more often. Otherwise, what you would need is a massive amount of memory for the system to cache that kind thing, which is just impractical. So here, we have NAND flash doing that exact job. It is very responsive and very dense. So for those reasons, basically, like if you have a huge deep learning workload, this is pretty much the premier solution right now, as far as I can tell. Both NVIDIA and Pure Storage seem to think so. We're looking at here is an example of an NVIDIA DGX A100 in action. Right now, this is actually computing deep learning data and it's connected via 100 gig per per second links through these switches here to the Pure Storage arrays. So there's the previous generation FlashBlade. And I believe there's also a FlashBlade//S there as well, which the third generation. I think, they're doing performance testing on those right now to see which is faster and how to basically optimize for these workloads. It's pretty amazing that we're basically at a point where SSDs aren't fast enough and yet the SSDs they're replacing them with are technically slower, because they're using QLC memory instead of TLC, which is just mind boggling. It's supposed to be an order of magnitude, less efficient, and yet they're making it work. Now, you might be thinking to yourself, these are full computers in these blades and server chassis that are basically just being used as SSD arrays. Like what happens after the SSD array is retired, like when you no longer need it or when you upgrade to a new one? Is the whole chassis just kind of thrown out? Well, no. What Pure Storage is doing is they have whole bunch of completely empty chassis here running virtual machines and other workloads that don't require that kind of storage. So they're repurposing those Xeon processors that would otherwise just be, I don't know, like if you retired an SSD and put it on yourself, what does that make it? So in this case, these live on even if the flash has died or it's been upgraded because it was too small in capacity. And in fact, behind you there are a whole bunch of processors and RAM and other stuff that's just sitting there waiting to be tested or reused. Now, you might be wondering how you even manage to talk to these things. Well, the file system that they use, I say file system very loosely, it's actually more like a database that you can actually then create what they call authorities on top of it with 128 of the split across the entire array. The larger the array is, the faster they go. All of those authorities can be used to do things like create object stores like Amazon S3. And from there, you could also create SMB. So Samba, Windows file sharing, or NFS for Linux file sharing support. So pretty much, it doesn't look any different to the end user. You can use it however you feel like you need to use your data. So you can pull stuff directly down through Amazon through your cloud services deployments. Or you can just use it as network storage if that's what you really want. And what's really cool, they probably wouldn't let me do it, but if you take out one of the DFMs and like rearrange it or something, within 10 minutes, it picks right back up without having to do anything special. You just slot I back in and it's like nothing happened. So you can, say for example, if you've got a blade that's misbehaving or you wanna upgrade it, you can completely migrate over without having to change anything about your configuration. Your users basically won't know what happened, because nothing will have happened. It's kind of magic. It's really, really warm back here. But these switches we were looking at earlier with all of these chassis plugged in, this is one of them. Not only is this a network switch but it's also basically the same type of thing you find in a FIOM in the back of a FlashBlade//S. So it's got an x86 processor here that handles all of the communications. And in fact, when you plug in multiple FlashBlade chassis, this thing takes over and actually orchestrates the entire model. So they're no longer doing their own individual arrays. This thing takes over automatically. Also, each one of these ports can do 40 or 100 gigabits per second, depending on the FlashBlade chassis. The older models can only do 40, whereas, the newer model FlashBlade//S, that can do 100. So it's a massive amount of data that will flow between this thing and, well, the rest of the rack. - Now, that you've seen the tech, let's talk about who these guys even are. Pure Storage was founded in stealth mode back in 2009 and debuted in 2011 as one of the first companies to introduce all flash infrastructure solutions in the industry. In the early days, they used consumer grade SSDs, which (chuckles) if you look back on the state of SSDs back then was pretty ballsy. But the solution was always software-driven. And they quickly began developing their own flash modules, which started shipping in 2015. Fast forward to today, and they partnered with companies like Cisco and NVIDIA with clients across the globe. So big thanks to Pure Storage for sponsoring this video and letting us show off their gear. You can learn more and maybe deploy one of these for yourself, if you're a straight baller or if you're an IT manager at the links below. Thanks for watching, guys. Maybe go check out one of the Intel design center tours that Linus did a little while ago. Like those are really, really next level in terms of how behind the scenes we're seeing, like Intel basically never lets anybody see that kind of stuff. And I'm glad that we got to see this kind of stuff here.",
    "transcript_keywords": [
        "Pure Storage",
        "Storage",
        "SSD",
        "Pure",
        "data",
        "Pure Storage arrays",
        "flash",
        "array",
        "SSDs",
        "NAND flash",
        "basically",
        "kind",
        "DRAM",
        "FlashBlade",
        "back",
        "SSD controller",
        "Pure Storage secret",
        "thing",
        "data center SSD",
        "stuff"
    ],
    "transcript_entity_values": [
        "40",
        "A100",
        "one",
        "less than a thousand",
        "tens of milliseconds",
        "second",
        "U.2",
        "Apple Silicon",
        "four",
        "128",
        "first",
        "100",
        "SSD",
        "up to 10",
        "48 terabytes",
        "California",
        "SD",
        "DGX",
        "last year's",
        "several decades",
        "fed",
        "Only four",
        "5U",
        "another 10 years or",
        "GPU",
        "DFM",
        "FIOM",
        "2009",
        "today",
        "NVIDIA",
        "2011",
        "2015",
        "up to",
        "four kilobytes",
        "Amazon S3",
        "Samba",
        "1.9 petabytes",
        "this very day",
        "FlashBlade//S",
        "Sunny Mountain View",
        "SAS",
        "10",
        "Pure Storage's",
        "eight 100 gigabit",
        "PCI",
        "Meta",
        "RAM",
        "FlashBlade S",
        "FPGA",
        "48 terabyte",
        "Pure Storage",
        "48",
        "CPU",
        "TLC",
        "the early days",
        "FlashBlade",
        "Cisco",
        "Amazon",
        "third",
        "Linux",
        "Linus",
        "NAND",
        "10 years ago",
        "1.3 watts",
        "two",
        "the Pure Storage",
        "Intel",
        "100 gigabits",
        "2.4 kilowatts",
        "10 minutes",
        "Xeon"
    ],
    "transcript_entity_types": [
        "CARDINAL",
        "PRODUCT",
        "CARDINAL",
        "CARDINAL",
        "QUANTITY",
        "ORDINAL",
        "ORG",
        "ORG",
        "CARDINAL",
        "CARDINAL",
        "ORDINAL",
        "CARDINAL",
        "ORG",
        "CARDINAL",
        "QUANTITY",
        "GPE",
        "ORG",
        "ORG",
        "DATE",
        "DATE",
        "ORG",
        "CARDINAL",
        "CARDINAL",
        "DATE",
        "ORG",
        "ORG",
        "ORG",
        "DATE",
        "DATE",
        "ORG",
        "DATE",
        "DATE",
        "QUANTITY",
        "QUANTITY",
        "ORG",
        "ORG",
        "QUANTITY",
        "DATE",
        "ORG",
        "LOC",
        "ORG",
        "CARDINAL",
        "ORG",
        "QUANTITY",
        "ORG",
        "ORG",
        "ORG",
        "ORG",
        "ORG",
        "QUANTITY",
        "ORG",
        "CARDINAL",
        "ORG",
        "ORG",
        "DATE",
        "ORG",
        "ORG",
        "ORG",
        "ORDINAL",
        "GPE",
        "PERSON",
        "PRODUCT",
        "DATE",
        "QUANTITY",
        "CARDINAL",
        "ORG",
        "ORG",
        "QUANTITY",
        "QUANTITY",
        "TIME",
        "ORG"
    ]
}