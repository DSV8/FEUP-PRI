{
    "id": "n_izpaZ0u5o",
    "title": "We’ve NEVER done this before… - Mother Vault Part 1 - JBOD",
    "channel": "Linus Tech Tips",
    "channel_id": "UCXuqSBlHAE6Xw-yeJA0Tunw",
    "subscriber_count": 15900000,
    "upload_date": "2022-07-03T17:11:16Z",
    "video_url": "https://www.youtube.com/watch?v=n_izpaZ0u5o",
    "category": "Science & Technology",
    "tags": [
        "the mother vault",
        "holy shit server",
        "holy server",
        "the vault server",
        "the biggest server ever",
        "supermicro",
        "jbod"
    ],
    "views": 2041698,
    "likes": 65969,
    "comments_count": 2426,
    "description": "Get $25 off all pairs of Vessi Footwear with offer code LinusTechTips at   Get your Crucial DDR5 RAM today at:   We've built some cool machines on the channel, but nothing is even close to the plan for our 3.6-petabyte archival storage array.   Discuss on the forum:   Check out the SuperMicro JBOD:  Check out InfiniteCable's MiniSAS HD external cables:   Purchases made through some store links may provide some compensation to Linus Media Group.   GET MERCH:   SUPPORT US ON FLOATPLANE:   AFFILIATES, SPONSORS & REFERRALS:   PODCAST GEAR:    FOLLOW US  ---------------------------------------------------   Twitter:  Facebook:  Instagram:  TikTok:  Twitch:   MUSIC CREDIT --------------------------------------------------- Intro: Laszlo - Supernova Video Link:  iTunes Download Link:  Artist Link:   Outro: Approaching Nirvana - Sugar High Video Link:  Listen on Spotify:  Artist Link:   Intro animation by MBarek Abdelwassaa  Monitor And Keyboard by vadimmihalkevich / CC BY 4.0   Mechanical RGB Keyboard by BigBrotherECE / CC BY 4.0  Mouse Gamer free Model By Oscar Creativo / CC BY 4.0   CHAPTERS --------------------------------------------------- 0:00 Intro & History 4:19 The JBOD 9:15 The Computer 14:10 FILLED 15:26 Cabling & Zoning 16:23 It's... LOUD 18:19 IPMI 18:54 What about dual path & high availability? 19:58 It's aliveeee! 21:30 Performance testing & outro",
    "description_links": [
        "https://www.Vessi.com/LinusTechTips",
        "https://crucial.gg/LTT_DDR5",
        "https://linustechtips.com/topic/1440919-the-craziest-server-weve-ever-built/",
        "https://lmg.gg/jbod",
        "https://lmg.gg/minisashdexternal",
        "https://lttstore.com",
        "https://www.floatplane.com/",
        "https://lmg.gg/sponsors",
        "https://lmg.gg/podcastgear",
        "https://twitter.com/linustech",
        "http://www.facebook.com/LinusTech",
        "https://www.instagram.com/linustech",
        "https://www.tiktok.com/@linustech",
        "https://www.twitch.tv/linustech",
        "https://www.youtube.com/watch?v=PKfxmFU3lWY",
        "https://itunes.apple.com/us/album/supernova/id936805712",
        "https://soundcloud.com/laszlomusic",
        "https://www.youtube.com/watch?v=ngsGBSCDwcI",
        "http://spoti.fi/UxWkUw",
        "http://www.youtube.com/approachingnirvana",
        "https://www.instagram.com/mbarek_abdel/",
        "https://geni.us/PgGWp",
        "https://geni.us/mj6pHk4",
        "https://geni.us/Ps3XfE"
    ],
    "transcript": "- Yee-haw! This is one buckin' bronco of a server! Okay. That didn't work. But, it doesn't matter because we have a serious problem. And this, (box thuds) this is one serious solution. In spite of all the issues we encountered with our archival storage system, dubbed \"The Vault.\" We've actually been able to recover nearly all of the data spread across the 2.4 petabytes of raw storage that made it up. Except that, because of the recovery process and the sheer amount of capacity required to hold it all, that data is now scattered to the high winds across the five discrete servers that are required to hold it. Some of which are getting close to a decade old. We had no desire to return to the admittedly imperfect setup we had before. Which means it is time for. (grunts) - [Jake] Whoa! - The mother vault! So called because this is one giant mother. The final deployment is gonna be not one, not two, but three of these. Each capable of housing nearly two petabytes of hard drive based storage. Is it time to bring back holy (beep)? - Did it ever go away? - No, it didn't just like our sponsor. Crucial. Don't just work faster, work better. When you need to open transfer or download files faster, get some great RAM with less lag time and more efficiency with Crucial. Get your Crucial DDR5 RAM today at the link down below. (upbeat music) (knife scratching) Before we get to the new stuff, we're gonna have to get you guys up to speed. Back in 2015, we built the first petabyte project or vault cluster. It was composed of two 45 drives 60 bay Storinator servers holding 120 10 terabyte hard drives, giving us a total capacity of 1.2 petabytes raw. These were combined into a single network share using a file system called GlusterFS. A few years later, we built new vault or petabyte project two. Which used 75 16 terabyte drives across two more 45 drive Storinators. Again, they were joined together with Gluster. However, since those two clusters were built several years apart, we ended up setting them up as two discrete clusters. That meant we had two separate network storage shares, which you can kind of think of as having two separate drives in your system. So if you wanna find a file and you don't know which drive it's on, chances are you're gonna have to search through them separately. This was fine at the time, 'cause we could usually guess which server a project was on based on how old it was. And honestly searching just two sources is not that difficult. However, if we went to scale that up again and build a third, eventually a fourth cluster, now it would start to get pretty annoying. Not to mention inefficient for the staff who have to use the freaking thing. The other issue is that managing even a single system takes a good amount of time. And every time we scale it up in this manner, it's another set of components. CPU, motherboard, RAM, power supplies. Ah, oh my God, this is really heavy. - Well that's the heavy side. - It's 130 kilograms. - I think that's when it's full. (laughs) - Holy crap! - [Jake] You ready? - Oh my God - [Jake] I told you you had the heavy side. - Yeah, but what the hell? Whoo! Anyway, hey more hardware means more potential issues, more potential service. So with that in mind, we've been looking for an alternative solution for some time. And while we could have continued to use GlusterFS and expand the existing clusters, we just don't really have anyone on staff who is familiar with it. And it's not that user friendly. I mean, heck even 45Drives, the company who provided us with the original petabyte project have actually switched primarily to a different cluster solution called Ceph. Now Ceph is cool. Really cool. We use it for Floatplane for example. And it's a bit better for scaling up, but it shares a lot of the same problems of our old setup. We don't have a ton of people on staff who are experienced with it, and would have the same or similar maintenance requirements. So instead we've opted to switch to a configuration that many home labs and data centers adore. One I personally have actually never used. (box scuffs) the JBOD. (laughs) You can fit so much hard drive in here. (laughs) This magnificent beast is the Super Micro 947HE1C-42K05JBOD. It's a 90 bay chassis that as the name JBOD implies, is quite literally just a bunch of discs. There's actually (box thuds) no computer in there. Which raises the question, what is here in the back where the compute module or multiple compute modules would normally be? This is it! This is what passes for the computer in this chassis. And there is some computing going on here. We've got some kind of ASIC that is, I mean, is this an FPGA even? [Jake]- I'm not sure it would be for SAS though. - Yeah. Could be. Are they doing enough, oh- - [Jake] Oh gosh. Let's- - I wanna see it. - [Jake] You don't wanna break it. - No, I don't wanna break it. I just wanna see it. - [Jake] There's a button cell battery in there. - [Linus] Is there? - This chassis does have IPMI. So that's what the battery would be for keeping track of time. So there's gonna be some compute for that, probably on the bottom board, but the rest of it should just be SAS stuff. - There's that IPMI management port, and then the only other IO on this machine is these six mini SAS, external ports. What are these? 88? - So each of these is four lanes of 12 gigabit SAS. So that's 48 gig per port. And you can use up to three at one time for connection to the head server. Which is, what is it? A 144 gigabit. - Okay. - That's - Casual. - Perfect then. - Oh, you can see the ASPEED chip for the IPMI right there. See it in there? There'll be some non-volatile memory on there too somewhere. - Might actually be this that looks like a NAND package. These are some chungus cooling fans. Look how thick they are. - [Jake] There's two in there, right? - 80 Millimeter fans. I actually don't think so. It's dual blade, but no, I think these are individual fans, Jake. Oh no, it's gotta be two. It's gotta be two. There's two discrete connectors. Either way, I guarantee you they absolutely rip. (Jake laughs) - [Jake] And then aside from, I guess these are probably the SAS and that would be your power. That's pretty much it. Very, very simple. And that's with intention. It's designed to be reliable, right? - Our other module here is, well it doesn't have a whole lot going on to be perfectly honest with you. What on earth does this do? You've got a small controller board that has a couple SATA ports, a couple NVME ports. And, more power and whatever the crap this is. This is probably PCIe. - I think when you buy the computer version of this. - Yeah. - These two NVMEs would come over here somewhere. If you just had a single compute node. But I'm not really sure. - Okay. - You could have a JBOD that's something called dual path. Now this is a single path, which means there's one path for the data to go from the drives to whatever computer it's plugged into. Because remember, there's no computer in here. It's gotta plug into a different computer. - Yes. - But in a dual path, there's two connections and you'll actually have redundant SAS expanders, which is the part that splits into multiple drives. You'll have redundant connections. So there'll be six more back here. And you'll have another IPMI. And the reason for that is you can have a high availability controller. So you can have controller one and controller two. And if controller one had a problems, it would just fail over to controller two. And you'd have better availability. - The problem, is that in order to take advantage of that, we need to use SAS drives. (Jake sighs) And that's a hassle. - Well, it's not only is it a hassle, but we already have 3.6 petabytes of SATA drives. - That's a hassle. - That's like $150,000 hassle. (both sniggering) - Not doing it. (Jake laughing) - Now let's take a look at the SAS expanders that make this magic happen. Naturally they're hot swappable 'cause they're servers. And in a nutshell, each of these takes in four of those SAS 12 gigabit per second lanes. And then is able to split that bandwidth to up to 30 flippin drives. So we only actually need three of these to populate the entire 90-bay capacity of this JBOD. By the way, Jake was talking about how you could have dual path. Well, you'd need three more expanders then, but we don't need them. Jake, do you have any idea what these connectors are called? - I have no idea. - Shout out them because these things are flipping cool. Look at the way these are joined back onto the board. That is dense with two S's. - [Jake] That's, oh my God. The pins on the bottom. - Right? - Holy crap. - The next thing I need Jake is for you to get me the head. - It's right there. - Right here. You can think of your JBOD kind of like an external hard drive. Or in this case, many, many external hard drives. Which means, that as Jake alluded to before, we need to have a computer somewhere. Well, that is where the head or the controller server comes into play. This particular machine is a placeholder. Super Micro's actually sending us over a dual AMD EPYC Milan server that's a little more tailored for this kind of application. So, instead of being full of NVMe drive bays in the front, it's got its PCIe lanes allocated to allow for lots of HBAs and network connectivity in the back. Because hard drives may be slow, but when you hook up enough of them, you can be pushing some serious freaking data. And this is gonna be connected to up to 270 drives? - About 255, I think is what it works out to. - 90 times three is 200 and- - But we don't have 90. Oh, but yeah I mean, you could- - You could add more drives, it's just JBOD. So cool right? In the meantime, for the purposes of our demo, we've loaded this machine up with three of these Broadcom HBA's. These things actually contain very little logic, compared to RAID cards, which had to perform parody calculations on the data running through them, these do almost nothing. They just take your bandwidth from your PCIe slot, in this case, PCIe 8X Gen 4 slot, run it through a SAS controller. And then that breaks out into 4, 8, 12, 16 SAS connections. - Do you wanna see my wonderful (stutters)? Blegh. Do you wanna see my wonderful card...? (beep) Do you wanna see my wonderful cardboard? - Yeah, that was worth that joke. - This math actually makes a lot of sense. If you have 90 drives and we'll say optimistically, they're running at 250 megabytes per second. - [Linus] Yeah. Maybe. - That's 22 and a half gigabytes a second. It's pretty fast. Conveniently three sets of 48 gigabytes of SAS cables is around 18 gigabytes a second. And even more conveniently, PCIe Gen 4 by eight. It's roughly 16 gigabytes a second in either direction. - So we're gonna be giving up some of the theoretical, maximum performance of our drives. But given that this is only under perfect conditions. - Yeah. - With brand new high performance drives - With nothing on them. - Reading and writing sequential data. - With nothing on them (snickers) - This'll be lost. - Yeah. (both giggle) And that's times three, because we're gonna have three JBODs. We're never gonna get anywhere close to this, maybe one gigabyte a second. - I'd like to point out something else that's convenient. - What? - Check this out. See that ratchet back pressure? - Wow. - I can start a screw with it. - Wow. - And, in spite of the silver shaft, look at that. I used it and my dick didn't fall off. (Jake snickers) - I actually think the silver shaft looks better. Wow! - Right? (Jake laughing) He's actually been one of - Did you do this one already - The bigger skeptics internally about this whole project. - I'm not a skeptic. I'm just a little bit critical. - But that ratchet? - I want it to be good okay. - You can sign up for a notification when it comes in stock lttstore.com - Try to sound less stressed about it. (beep) - In the meantime we also have shirts and stuff. Cash flow's a little tight. Now you might look at this machine and think that's ludicrous overkill. Surely, the one that Super Micro is gonna send is a little more pedestrian. But that's not actually the case, even though we're not running super high speed NVMe drives. When you're hooking up hundreds of hard drives. - With the potential to expand to hundreds of more. - You need CPU and RAM galore. More than you'd think. - In fact, we actually have two 32 core EPYC 75F3 processors. Those are the frequency oriented, 280 watt per CPUs. And we're gonna have around a terabyte of RAM. With the potential to expand that down the road. Because, we will be using ZFS, and ZFS greatly benefits from RAM for re-caching. We'll also have NVMe drives and stuff, but that's TBD. You'll have to get subscribed for the next video. When we actually deploy the whole thing. - Are we doing tiering? - No, oh, well maybe one day. It's possible. we'll let in our a secret. We might build a highly available WANIC, and set this up as a tiered archival layer. Entirely transparent to the user. - So just one drive? - Yeah. - Letter? - One drive. - Oh my God. These are just, these are just drive sleds. I thought these were boxes of hard drives Jake. - No, those are the trays. - I've been deceived. - Do you wanna see what they tray like? - We were talking about how we wanted to discuss. Yeah, we also need the high performance CPU for ZFS compression. I was like, oh, well we'll just dump a bunch of drives in here. And then we'll talk about okay, we've got this much capacity. But we're gonna use compression. And then Jake's like, \"Oh, we've only got 13 drives to put in it for the demo.\" Come on, Jake. It's almost like all our hard drives are deployed. Holding data temporarily from the old vault. - Yeah. Almost. - Okay. Well I guess it's a 13 drive demo. And that seems lucky. - If you want- - If you want Ed to yell at you, we could take a part in Delta 5. That's got new vault on it. I think. I don't want Ed to yell at me. - And some of old vault. - When Ed yells at me... - Wendell's still working on Delta 3. So that's not available. Temp vault, we could do temp vault, but then Ed would also yell at you probably. - It's not fit, 13 drive seems fine. - All right, we found another solution to some test hard drives. This here Storinator was full of 60 Spin-erenos. (box clatters) And now they're in here. It has made the balance of this system super sketchy. Like, oh, it gets worse. Look, look at that. Look it's... (laughs) Oh God. (grunts) I can't reach it. Okay, there we go. But there you go. There's 60 hard drives. And they're kind of in a weird orientation, because there's a kind of a poop mix of drives in here. There's some 10s' there's some 12s', there's some 16s'. I think there's even one 20. So I just wanted to kinda separate those but this is our test. Now these 60 drives do already have an array on them. And I think it's about 70% full. This is some of our existing archival data. But they're gonna be set up as four vdevs that are RAID Z2, 15 drives wide. Not great for performance. I don't think we're gonna see anything crazy in terms of numbers out of this machine, but it is kind of indicative of what we're gonna have it set up as. We're probably gonna switch to 10 drive RAID Z2s' just to make it a little bit faster. But this should still be pretty dang fast. Now in order to get a connection between our server and our JBOD, we have these cool mini SAS HD external cables from Infinite Cables. I think Super Micro also sent us some. By default, this JBOD's configured as a single zone. Now you can do kind of cool stuff with zoning in a JBOD. For instance, you can split this up into two zones and have two controller servers, being serviced by one JBOD. So you'd have 45 drives on either. You can also do three zones, so that'd be 30, 30, 30. You can do three controller servers, but they can't access the other zone's data. You'd only get the zone that you're actually physically attached to. So you'd have A, C or E, on the back of this server. Now, in our case, we're gonna be using three zones, but to one controller server. And the reason for this, is it allows us instead of getting one cable, which is 48 gigabit of bandwidth, we now get two and then three for 144 gigabit. Which again is probably overkill but what do we do that's not overkill? (giggles) Hey, you might need these. Like for realsies. (power supply whining) No, no, just wait. - This is just the power supply? - That's the power supply. That's like full tilt though. (server whining) - Whoa! This is ludicrously loud. - So I put it in the server room with the door shut. You can hear it in the bedroom. (power supply whining) But this is like, they're actually at 100% right now. - They should never run at that speed. - Never - And with the 60 drives, I already tried it, like running a load. And they barely even spin up. - Okay. That's good. - Yeah. I got a little worried. (laughs) - If the air conditioning ever fails in the server room We'll know! 'Cause we'll hear this everywhere. - There we go, oh!. Give it a sec. - Okay. - Coming back to planet Earth. - That's a lot more reasonable. I mean, I guess that's what fans that thick'le do for you. - Well, I mean, when you have 90 hard drives, you're talking like at least 1,000 watts. - Yeah. - That's a lot of power. - I mean, what are these power supplies? - They're 2,000 watt on 208 volt. - 2,000 watt power supplies? - So to have like actual power supply redundancy, you need to be running 208 volt. - Wow. - Otherwise you'll probably use enough power that it needs to split across the two. So we might actually have to rewire our server room to be 208 volt. Because for some reason it wasn't wired that way in the first place. I don't know - I don't remember why Brian told me- - There's probably a reason. - Yeah. There's a reason. - And I bet you, there's a way to fix it. - Oh my God! Yeah, you can fix anything with money. - You have lots of that right now, right? - No. - Oh, well that's fine. We'll figure it out. - You know what? No, we should just put these in lab two. Because that one will be 208 volt right out the gate. - Oh it'll make it too loud though. - Time to put more insulation on the walls. - Jesus. - With nails. - Yeah. I'll show you that quick config here. This is where you can change the zone. So there's single zone, two zone or three zone. Again, like I explained before, you could use that for multiple controllers or to increase throughput. We're doing the throughput route of course. That's pretty much all you can do in here. There's no remote control. Like, there's no screen. It doesn't have a display output it's just - Right, yeah! - Power. - It's just a big, dumb, big discs. (Jake laughs) - [Jake] System Critical I don't know why. Probably 'cause you pulled the power supply out. - Yeah, that was probably it. - Plug it back in. - I'm sorry about that buddy, here you go. Yeah, there's your medicine. Right in the butt, suppository medicine, power cable. Now tell me something. - Yeah? - If we decided to go high availability in the future, nothing would actually prevent us from adding a second interface unit back there. - Yes. - A second head server. - Yes. - No problem right? - We need SAS drives. - But sorry not high availability at the... Oh, 'cause dual path. Aw, crap. Okay yep. - The thing about this approach though is, we've already proven, we don't really depend on this. - Yeah. - It's been like, it's gotta be like eight months at this point- - Yeah! - That we haven't had it. - It's funny how many people were listening to us in the video saying, \"Yeah, it's really optional retaining this data. We lost it because we don't really care that much.\" And they were like \"Lies.\" like, no, we... - We Haven't had it for like almost a year- - Yeah. - At this point and it's been totally fine. It is nice to have, but here's the thing. - It is nice to have. - If this controller server were to poop the bed, or to have some problem. - Well everything here should still be fine. - We can just unplug this. And we'll have some other server temporarily. - So like a cold spare. - Put the... yeah, basically. - Cold spare server. - I mean, you're talking maybe an hour of downtime, like realistically. - I was looking at this and I thought maybe I saw four vdevs. I thought that was the four drives. I just saw it outta the corner of my eye and I thought you had put the 30 terabyte NVMes in here. - No, no. We might though. Wendell's been talking about this special metadata device for a while. And basically he was saying he has like half a petabyte with 24 terabytes of special metadata device and ZFS. And it filts. So I'm like, we might as well just put four 30 terabytes in there and that'll give us 60 terabytes. - What does special metadata do? - So, you know like file metadata and like the directory structure? - Does that make searching really fast? - It makes searching way faster. - Ooh, that's cool. - [Jake] So it stores metadata, like where files are, and he's talking about it right here. - It's literally Wendell's- - Literally Wendell. - Blog post. It'S the first thing that comes up. Hi, Wendell. - Yeah. Yeah. But here's the thing is we can have a level two arc. We can have a log device and we can have special metadata. - Special metadata. - Yeah. - So he's saying 172 terabytes of space is 5 terabytes of metadata. But here's the thing. We don't have to have all of it metadataed in there. Like it'll pull it when it needs it, right? - Okay. - [Jake] Yeah, by default, this includes all the metadata, the indirect blocks of user data and any de-duplication tables stored on that device. So maybe de-duplication. - If you are running de-duplication, this is probably really important to improving performance. - Yeah. But either way, we'll try it. As you can see, it's about 70% full. Not ideal for performance. And on top of that, it's an existing pool that it's set up in four Vdev's right? - Yeah. - So they're 15 wide. - Yeah. - So really I'm expecting like maybe three to five gigabytes a second. - Okay. - Sequential. Like, I would not be surprised if that's where we end up. Because when we move to the final deployment, we're gonna do 10 wide which will be better for performance. - Yup. - And we'll do 10 wide RAID Z2's. Which is basically the same as 15 wide RAID Z3's. - Yup. - And then we'll obviously have a lot more drives. - If you've got 10 drives of which you could lose 2 of them before you any data loss. That's kinda the same as having 15 drives of which you could lose three, in terms of the ratio. - But then the whole pool, if you look at it from a wide perspective. - But it's probably slightly better to have more vdevs, if you run the probability analysis. (Jake giggles) - Anyway, so this. - No. - No, no, no. We gotta explain. The data set is set up with caching only metadata, which is what we want. The RAM cache is turned off. Oh let's, give it a second. There we go again like... let's just give it, you know, just warm up there, bud. C'mon. - Okay. That's about two gigabytes a second. - Come on, you can do better than that. I didn't even look at- - No, I don't think it can. - Come on three, (grunts) three. - Yeah, It's not three sustained, that's for sure. - 3.6. - You can't just read the high numbers. - I'm reading just the high number. That was 3.7 there for a second. - Yeah, it really doesn't work like that. That's 579. - Whoa, there's 4,000 for a second there. - Yeah but there's 0.3. (Jake laughing) - Probably has something to do with the fact that the drives are pretty full already. And remember we have no NVMe caching. We have no RAM caching. Like, this is raw dog giant vdev just disk. - Now tell me something, hold on a second, wait. Here's something I don't fully understand. This is really cool for maintenance, the fact that you can have the system running, you don't have to pull all your cables out at the back in order to slide the drives zone. - Yeah, the cables staying in the same spot, super nice. - But how is all of this still connected? These drives are blinking. These are doing things. Oh, it's just like a ribbon. - [Jake] Yeah, giant ribbon cable on the back. You see it back there? - Oh, that's super cool. Man, that's amazing for maintenance. So you can just go be like, okay, yeah, I've got a bad drive in bay whatever. - Pull this out- - Open it up. - And you don't have to slide any servers out. - See you later. - [Jake] 'Cause this is internal, right? You just pull it out and you grab that one. We're gonna have to label them. - Oh yeah. - [Jake] Or I might just do a spreadsheet, honestly. - Yeah, that would be fine. - If they were SAS drives, you have like SAS enclosure service or something like that. SES, whatever, anyways, you can go and be like, what's in that slot or by drive, you can say what slot is that drive in? And it'll tell you. But since they're SATA drives, they'll all just show as not connected. (laughs) Let's put this back in. This is making me anxious. - Let's do that. I'm like I was- - Oh boy. Okay. - Yeah. - So we're doing 16 jobs in an IO depth of 16 block size one meg. It's gonna be probably basically the same. Ah 2000. Whoo! Blow my skirt up. - More consistent though. - Yeah, like- - Nope. There it goes. See you later. - In the grand scheme of things, we're gonna be writing to this thing, like basically peak one gigabyte a second. - Yeah. - Ever. And it will be better when there is more vdevs. - Okay, then I think we're good. Good to tell you about our sponsor. - Vessi! Vessi proves that waterproof shoes don't need to be ugly or uncomfortable. Thanks to their Dyma-tex technology, you can keep those toes dry while remaining sleek, snug and stylish. Need even more comfort and breathability? The Everyday Move line has added padding at the mid-sole, and a looser knit to keep up with your busy, active lifestyle. Do you hate tying laces? Tired of chasing the rabbit around the tree and down the hole? Well, fear not, because Vessi's Everyday Move comes with handy pull-tabs for easy slip on action, and are made 100% creature free. So you can be walking around on fluffy clouds with a clear conscience all day long. And, when Vessi says Everyday Move, they mean it, hot, cold, wet, dry, stay comfortable, and in any weather. Just think of all the freedom you can get with your new shoes. So treat those feet of yours to Vessi Everyday Move shoes and save $25 with offer code Linus Tech Tips at Vessi.com/LinusTechTips. If you guys enjoyed this video, you might also enjoy the one where we explain what happened to the old vault and how we lost nearly all of the data on it. - What? We got it all back. - Well, yeah, I know. But first we lost it. - Well, like sort of.",
    "transcript_keywords": [
        "drives",
        "Yeah",
        "SAS",
        "hard drives",
        "drive",
        "SAS drives",
        "server",
        "back",
        "JBOD",
        "data",
        "power",
        "controller",
        "hard",
        "RAM",
        "Jake",
        "time",
        "Vault",
        "hard drives Jake",
        "kind",
        "metadata"
    ],
    "transcript_entity_values": [
        "one gigabyte",
        "S's",
        "one",
        "120 10 terabyte",
        "EPYC",
        "four 30 terabytes",
        "30,",
        "13",
        "Broadcom HBA's",
        "like half a petabyte",
        "Vessi",
        "3.7",
        "eight",
        "One",
        "Floatplane",
        "RAID Z2's",
        "130 kilograms",
        "3.6",
        "0.3",
        "A few years later",
        "Storinator",
        "three",
        "Linus",
        "at least 1,000 watts",
        "NVMes",
        "About 255",
        "NAND",
        "Earth",
        "1.2",
        "SES",
        "Gen 4",
        "250 megabytes",
        "Delta 5",
        "first",
        "25",
        "Ceph",
        "Lies",
        "32",
        "208 volt",
        "one day",
        "Vessi Everyday Move",
        "AMD EPYC Milan",
        "2015",
        "Delta 3",
        "24 terabytes",
        "3.6 petabytes",
        "Storinator",
        "JBOD",
        "Super Micro's",
        "SAS",
        "Jake giggles",
        "4",
        "Linus Tech Tips",
        "CPU",
        "six",
        "sec",
        "third",
        "ASPEED",
        "579",
        "60",
        "hundreds",
        "about 70%",
        "280 watt",
        "around 18 gigabytes",
        "Dyma-tex technology",
        "4,000",
        "Ceph",
        "four",
        "two 45",
        "150,000",
        "about two gigabytes",
        "Vdev",
        "SAS HD",
        "the Super Micro 947HE1C-42K05JBOD",
        "45",
        "several years",
        "ZFS",
        "2.4 petabytes",
        "Infinite Cables",
        "Everyday Move",
        "FPGA",
        "2,000 watt",
        "16",
        "five",
        "10",
        "a ton",
        "close to a decade old",
        "30",
        "Jake]-",
        "Super Micro",
        "Wendell",
        "100%",
        "15",
        "petabyte",
        "48",
        "Gluster",
        "12",
        "up to 30",
        "up to 270",
        "fourth",
        "two",
        "30",
        "Ed",
        "ASIC",
        "Jake snickers",
        "second",
        "172 terabytes",
        "Jake",
        "12",
        "Brian",
        "WANIC",
        "75 16 terabyte",
        "PCIe Gen 4",
        "The Vault",
        "80 Millimeter",
        "an hour",
        "48 gigabytes",
        "today",
        "three to five gigabytes",
        "SATA",
        "RAM",
        "roughly 16 gigabytes",
        "200",
        "90",
        "75F3",
        "5 terabytes",
        "22 and a half gigabytes",
        "2",
        "12 gigabit",
        "eight months",
        "NVME",
        "8",
        "144",
        "GlusterFS",
        "IO"
    ],
    "transcript_entity_types": [
        "QUANTITY",
        "ORG",
        "CARDINAL",
        "QUANTITY",
        "ORG",
        "TIME",
        "DATE",
        "CARDINAL",
        "ORG",
        "CARDINAL",
        "PERSON",
        "CARDINAL",
        "CARDINAL",
        "CARDINAL",
        "GPE",
        "ORG",
        "QUANTITY",
        "CARDINAL",
        "CARDINAL",
        "DATE",
        "ORG",
        "CARDINAL",
        "WORK_OF_ART",
        "QUANTITY",
        "ORG",
        "CARDINAL",
        "PRODUCT",
        "LOC",
        "CARDINAL",
        "ORG",
        "PRODUCT",
        "QUANTITY",
        "LAW",
        "ORDINAL",
        "MONEY",
        "ORG",
        "WORK_OF_ART",
        "CARDINAL",
        "QUANTITY",
        "DATE",
        "ORG",
        "ORG",
        "DATE",
        "LAW",
        "CARDINAL",
        "QUANTITY",
        "PERSON",
        "ORG",
        "ORG",
        "ORG",
        "PERSON",
        "CARDINAL",
        "WORK_OF_ART",
        "ORG",
        "CARDINAL",
        "ORG",
        "ORDINAL",
        "WORK_OF_ART",
        "CARDINAL",
        "CARDINAL",
        "CARDINAL",
        "PERCENT",
        "QUANTITY",
        "QUANTITY",
        "ORG",
        "CARDINAL",
        "PERSON",
        "CARDINAL",
        "DATE",
        "MONEY",
        "QUANTITY",
        "PERSON",
        "ORG",
        "ORG",
        "CARDINAL",
        "DATE",
        "PRODUCT",
        "QUANTITY",
        "ORG",
        "ORG",
        "LAW",
        "QUANTITY",
        "CARDINAL",
        "CARDINAL",
        "CARDINAL",
        "QUANTITY",
        "DATE",
        "CARDINAL",
        "PERSON",
        "ORG",
        "PERSON",
        "PERCENT",
        "CARDINAL",
        "CARDINAL",
        "CARDINAL",
        "PRODUCT",
        "CARDINAL",
        "CARDINAL",
        "CARDINAL",
        "ORDINAL",
        "CARDINAL",
        "DATE",
        "PERSON",
        "ORG",
        "PERSON",
        "ORDINAL",
        "QUANTITY",
        "PERSON",
        "DATE",
        "PERSON",
        "EVENT",
        "QUANTITY",
        "PRODUCT",
        "WORK_OF_ART",
        "QUANTITY",
        "TIME",
        "QUANTITY",
        "DATE",
        "QUANTITY",
        "ORG",
        "ORG",
        "QUANTITY",
        "CARDINAL",
        "CARDINAL",
        "CARDINAL",
        "QUANTITY",
        "DATE",
        "CARDINAL",
        "QUANTITY",
        "DATE",
        "ORG",
        "DATE",
        "CARDINAL",
        "ORG",
        "GPE"
    ]
}