{
    "id": "dwE0qMxCpbE",
    "title": "AMD CEO Lisa Su on the GPU shortage, the AI revolution, and Nvidia | Decoder",
    "channel": "The Verge",
    "channel_id": "UCddiUEpeqJcYeBxX1IVBKvQ",
    "subscriber_count": 3390000,
    "upload_date": "2023-09-29T14:00:33Z",
    "video_url": "https://www.youtube.com/watch?v=dwE0qMxCpbE",
    "category": "Science & Technology",
    "tags": [
        "amd",
        "lisa su",
        "lisa su interview",
        "lisa su amd",
        "amd vs intel",
        "amd ceo",
        "advanced micro devices",
        "amd ryzen",
        "amd ceo lisa su",
        "amd stock",
        "amd ceo interview",
        "lisa su cnbc",
        "interview with lisa su",
        "amd shares",
        "amd stocks",
        "amd rx 580",
        "amd radeon rx 580",
        "amd vs nvidia",
        "amd radeon",
        "amd gpu",
        "chips",
        "tech",
        "code",
        "code 2023",
        "code conference"
    ],
    "views": 85796,
    "likes": 2014,
    "comments_count": 208,
    "description": "At this years Code Conference, the CEO of one the worlds largest computer chip companies discusses competing with Nvidias leading GPU, AI regulation, and the global supply chain. Presented by @PolestarCars   Read more:   0:00 Intro 1:50 Interview 2:00 Lamini announcement 3:14 Chip market 5:24 CHIPS and Science Act 8:26 TSMC 8:50 Huawei seven-nanometer chip 10:11 Nvidia 13:40 CUDA and ROCm 15:44 PyTorch 18:03 Microsoft 18:50 Consumer prices and AI 20:55 Regulation 23:48 Gaming 24:24 Audience Q&A  The Verges sponsors play an important role in funding our journalism, but do not influence editorial content. For more information about our ethics policy, visit   Subscribe:  Like The Verge on Facebook:  Follow on Twitter:  Follow on Instagram:   Decoder Podcast:  The Vergecast Podcast:  The Vergecast on YouTube:  More about our podcasts:   Read More:  Community guidelines:  Wallpapers from The Verge:",
    "description_links": [
        "https://bit.ly/3ZEd5me",
        "https://www.theverge.com/ethics-statement.",
        "http://goo.gl/G5RXGs",
        "https://goo.gl/2P1aGc",
        "https://goo.gl/XTWX61",
        "https://goo.gl/7ZeLvX",
        "https://trib.al/iENzKtd",
        "https://pod.link/430333725",
        "https://www.youtube.com/thevergecast",
        "https://www.theverge.com/podcasts",
        "http://www.theverge.com",
        "http://bit.ly/2D0hlAv",
        "https://bit.ly/2xQXYJr"
    ],
    "transcript": "(slow upbeat music) - Hello and welcome to Decoder. I'm Nilay Patel, editor-in-chief of The Verge, and Decoder is my show about big ideas and other problems. Today, we're bringing something a little different. The Code Conference was this week, and we had a great time talking live on stage with all of our guests. We'll be sharing a lot of these conversations here in the coming days. And the first one we're sharing is my chat with Dr. Lisa Su, the CEO of AMD. Lisa and I spoke for half an hour, and we covered an incredible number of topics, especially about AI and the chip supply chain. These past few years have seen a global chip shortage, exacerbated by the pandemic, and now coming out of it, there's suddenly another big spike in demand. Thanks to everyone wanting to run AI models. \"The balance of supply and demand is overall in a pretty good place right now,\" Lisa told us, with the notable exception of these high-end GPUs, powering all of the large AI models that everyone's running. The hottest GPU in the game is Nvidia's H100 chip. But AMD is working to compete with a new chip Lisa told us about, called the MI300, that should be as fast as the H100. There's also a lot of work being done in software to make it so that developers can move easily between Nvidia and AMD. So we got into that. You'll also hear what Lisa talk about what companies are doing to increase manufacturing capacity. The CHIPS and Science Act that recently passed is a great step towards building chip manufacturing here in the United States. Lisa told us, it takes a long time to bring up that supply. So I wanted to know how AMD is looking to diversify the supply chain and make sure it has enough capacity to meet all of this new demand. Finally, Lisa answered questions from the amazing Decoder audience. We talked a lot about how much AMD is using AI inside the company right now. It's more than you think. Although, Lisa did say, \"AI is not gonna be designing chips all by itself anytime soon.\" Okay, Dr. Lisa Su, CEO of AMD. Here we go. (futuristic music) I have a ton to talk about, 500 cards worth of questions. (Dr. Lisa chuckling) We're gonna be here all night. But let's start with something exciting. AMD made some news today in the AI market. What's going on? - Well, I can say, first of all, you know, the theme of this, you know, whole conference AI, it's the theme of everything in tech these days. And when we look at all of these opportunities for, you know, computing to really advance AI, that's really what we're working on. So, yes, today we did have an announcement this morning from a company, a startup called Lamini, great company that we've been working with, some of the top researchers in large language models. And the key for everyone is, when I talk to CEOs, people are all asking, you know, \"I know I need to pay attention to AI, I know I need to do something, but, like, what do I do? It's so complicated. There's so many different factors.\" And with these foundational models like LLaMA, which are, you know, great sort of foundational models, many enterprises actually wanna customize those models with their own data and ensure that you can do that, you know, sort of in your private environment and, you know, for your application. And that's what Lamini does. They actually customize models, fine tune models for enterprises, and they operate on, you know, AMD GPUs. And so that was a cool thing. And we spent, you know, a bit of time with them, quite a bit of time with them, really optimizing the software and the applications to make it as easy as possible to develop these enterprise, you know, fine tuned models. - Yeah, I wanna talk about that software in depth. I think it's very interesting where we're abstracting the different levels of software development away from the hardware. But I wanna come back to that. I wanna begin broadly with the chip market, right? We're exiting a period of pretty incredible constraint in chips, sort of across every process node. Where do you think we are now? - Well, it's interesting. You know, I've been in the semiconductor business for, I don't know, the last 30 years. And for the longest time, people didn't really even understand what semiconductors were or where they were in, you know, where they fit in the overall supply chain and where they were necessary in applications. I think the last few years, especially with the pandemic-driven demand, and, you know, everything that we're doing with AI, people now are really focused on semiconductors. I think there has been a tremendous cycle, one, a cycle where we needed a lot more chips than we had, and then a cycle where we had too many of some. But at the end of the day, I think the fact is semiconductors are essential to so many applications, and particularly, for us, what we're focused on are the most complex, the highest performance, the bleeding edge of semiconductors. And I would say that there's tremendous growth in the market. - What do you think the bottleneck is now? Is it at the cutting edge? Is it at the older process nodes, which is what we are hearing sort of in the middle of the chip shortage? - Yeah, I mean, look, I think the industry, as a whole, has really come together as an ecosystem to put a lot of capacity on for the purposes of ensuring that we do satisfy overall demand. So in general, I would say that, you know, the supply demand balance is in pretty good place with, perhaps, the exception of GPUs. If you need GPUs for large language model training and inference, they're probably tight right now. (Nilay laughing) A little bit tight. (chuckling) - [Nilay] Why do you think you're here, yeah. - And- - Lisa's got some in the back if you need some. (Lisa chuckling) - But, look, the truth is, we absolutely are putting a tremendous amount of effort getting the entire supply chain ramped up. These are some of the most complex devices in the world. I mean, you know, hundreds of billions of transistors. Lots of, you know, advanced technology, but absolutely ramping up supply overall. - The CHIPS and Science Act passed last year, massive investment in this country in fabs, AMD, obviously, the largest fabless semiconductor company in the world. Is that had a noticeable effect yet or are we still waiting for that to come to fruition? - Well, I do think that if you look at, you know, the CHIPS and Science Act and what it's doing for the semiconductor industry in the United States, it's really a fantastic thing. I mean, I have to say, you know, hats off to, you know, Gina Raimondo and everything that the commerce department is doing with industry. These are long lead time things, right? The semiconductor ecosystem in the U.S. needed to be built, you know, sort of five years ago. It is being built, it is expanding now, especially at the leading edge, but it's gonna take some time. So I don't know that we feel the effects right now, but one of the things that we always believe is, you know, the more you invest over the longer term, you'll see those effects. So I'm excited about on for capacity. I'm also really actually excited about some of the investments in our national research infrastructure. 'Cause that's also extremely important for, you know, long-term semiconductor, you know, strength and leadership. - AMD's results speak for themselves. You're selling a lot more chips than you were a few years ago. Where have you found that supply? Are you still relying on TSMC, while you wait for these new fabs to come up? - Well, again, when you look at sort of the business that we're in, it's pushing the bleeding edge of technology. So we're always on the most advanced node and trying to get, you know, the next big innovation out there. And there's a combination of both, you know, process technology, manufacturing, design, design systems. We are very happy with our partnership with TSMC. They are the best in the world with, you know, advanced and leading edge technologies. But we also think- - But they're here, right? Can you diversify away from them? - Well, I think the key is geographical diversity, Nilay. So when you think about geographical diversity, and by the way, this is true no matter what. Nobody wants to be in the same place because there are, you know, there are just natural risks that happen. - [Nilay] Yeah. (chuckling) - And that's where the CHIPS and Science Act has actually been helpful, because there are now, you know, significant numbers of manufacturing plants being built in the U.S. They're actually gonna start, you know, production, you know, over the next number of quarters. And, you know, we will be active in having some of our manufacturing here in the United States. - I talked to Intel CEO Pat Gelsinger when they broke around in Ohio. You know, they're trying to become a foundry. He said very confidently to me, \"I would love to have an AMD logo on the side of one of these fabs.\" How close is he to making that a reality? - Well, I would say this, I would say that, from a onshore manufacturing, we are certainly looking at, you know, lots and lots of opportunities. I think, Pat has a very ambitious plan. And, (Nilay chuckling) and I think that's there. I think we always look at, you know, who are the best manufacturing partners? And, you know, what's most important to us is, you know, someone who's really dedicated to the bleeding edge of technology. - Is there a competitor in the market to TSMC on that front? - I think there are, there's always competition in the market. I mean, if you think about, you know, TSMC is certainly very good. You know, Samsung is certainly making a lot of investments. You mentioned Intel. I think there's some, there's some activities in Japan as well to bring up advanced manufacturing. So there are lots of different options. - Last question on this thread, and then I do wanna talk to you about AI. There's a lot of noise recently about Huawei. They put out a seven nanometer chip. This is either an earth-shattering geopolitical event or it's bullshit. (audience laughing) What do you think it is? - Let's see, I don't know that I would call it a earth-shattering geopolitical event. (Nilay laughing) Look, I think there's no question that, you know, technology is considered a national security, you know, of importance. And, you know, from a U.S. standpoint, I think we want to ensure that, you know, we keep that lead. Again, I think the U.S. government has spent a lot of time, you know, on this aspect. You know, the way I look at these things is, you know, we are a global company. You know, China's an important market for us. We do sell to China more consumer-related goods, you know, versus other things. And there's an opportunity there for us to really have a balanced approach into how we deal with some of these geopolitical matters. - Do you think that there was more supply available at TSMC because Huawei got kicked outta the game? - I think TSMC has put a tremendous amount of supply on the table. If you think about the CapEx that's happened over the last three or four years, it's there, because we all need more chips. And when we need more chips, the investment is there. Now chips are more expensive as a result, and, you know, that's part of the ecosystem that we've built up. - Yeah, let's talk about that part of it. So you mentioned GPUs are constrained, the Nvidia H100, there's effectively a black market for access to these chips. You have some chips. You're coming out with some new ones. You just announced, you know, Lamini's training fully in your chips. Have you seen opportunity to disrupt this market because Nvidia supply is so constrained? - Yeah, I mean, I would take a step back, Nilay, and just talk about just what's happening in the AI market 'cause it's incredible what's happening. I mean, if you think about, you know, the technology trends that we've seen over the last, you know, whatever, 10, 20 years, whether you're talking about, you know, the internet or the mobile phone revolution, or, you know, how PCs have changed things. Like, AI is like, you know, like, 10 times, a 100 times more than that in terms of how it's impacting everything that we do. So if you talk about enterprise productivity, if you talk about personal productivity, or, you know, society, what we can do from a productivity standpoint, it's that big. So the fact that there's a shortage of GPUs, I think is not surprising because people recognize how important the technology is. Now we're in such the early innings of how AI and especially, Generative AI, is coming to market, that I view this as, like, you know, a 10-year cycle that we're talking about. Not like a, you know, how many GPUs can you get in the next two to four quarters? We are excited about our roadmap. I think, you know, with high performance computing, I would call Generative AI, you know, kind of the killer app for high performance computing. You need more and more and more. And as good as, you know, today's large language model is it can still get better if you had, you know, you continue to increase the training performance and the inference performance. And so that's what we do. We build the most complex chips. We do have a new one coming out, it's called \"MI300\" if you want the code name there. And it's gonna be fantastic. You know, it's targeted at large language model training as well as large language model inference. And what we view is, you know, your question of, \"Do we see opportunity?\" Yes, I mean, we see significant opportunity and it's not just in one place. You know, the idea of the cloud guys are, you know, sort of the only users. That's not true. I mean, there's gonna be a lot of enterprise AI, a lot of the startups have, you know, tremendous, you know, sort of a VC backing around AI as well. And so we see, you know, opportunity across all those spaces. - So MI300? - MI300, you got it. - You heard it here first. Performance wise, this is gonna be competitive with the H100 or exceed the H100? - It is definitely gonna be competitive from, you know, training workloads type things. But one of the things that, you know, we've done, and in the AI market, there's no one-size-fits-all as it relates to, you know, chips. There are some that are gonna be exceptional for training. There's some that are gonna be exceptional for inference, and, you know, that depends on how you put it together. And what we've done with MI300, is we've built an exceptional product for inference, especially large language model inference. So when we look, going forward, much of what work is done right now is, companies kind of training and deciding what their models are gonna be. But going forward, we actually think inference is gonna be a larger market, and that plays well into some of what we've, you know, designed MI300 for. (futuristic music) - If you look at what sort of the, you know, Wall Street thinks Nvidia's mode is, it's CUDA, it's the proprietary software stack, it's the long-running relationships with developers. You have Ryzen, which is a little different. Do you think that that's a moat that you can overcome with better products or with a more open approach? How are you going about attacking that? - Yeah, I'm not a believer in moats when the market is moving as fast as it is. I mean, when you think about moats, it's more sort of mature markets where, you know, people are not really wanting to change things a lot. When you look at Generative AI, I mean, it's moving at an incredible pace. I mean, the progress that we're making in, you know, a few months in a regular, you know, sort of development environment might have taken a few years. And software, in particular, you know, our approach is an open software approach. There's actually a dichotomy. If you look at people who have developed software sort of over the last five or seven or eight years, they've tended to use, let's call it more hardware specific software. It was convenient. There weren't that many choices out there and so that's what people did. When you look at, you know, going forward, actually what you find is, everyone's looking for the ability to build hardware-agnostic software because people want choice. I mean, you know, frankly, people want choice. People wanna use their older infrastructure. People want to ensure that they're able to, you know, move from, you know, one infrastructure to another infrastructure. And so they're building on these higher levels of software. Things like, you know, PyTorch, for example, which tends to be that hardware-agnostic capability. So I do think, the next 10 years are gonna be different from the last 10 as it relates to how do you develop within AI. And I think we're seeing that across, you know, sort of the industry and the ecosystem. And the benefit of an open approach is that, you know, that there's no one company that has all of the ideas. And so the more we're able to bring the ecosystem together, we get to take advantage of all of those, you know, in a really, really smart developers who want to accelerate AI learning. - PyTorch is a big deal, right? This is the language that all these models are actually coded in. I talked to a bunch of, you know, cloud CEOs, they don't love their dependency on Nvidia as much as anybody doesn't love being dependent on any one vendor. Is this a place where you can go work with those cloud providers and say, \"We're gonna optimize their chips for PyTorch and not CUDA and developers can just run on PyTorch and pick whichever's best optimized? - That's exactly it. So if you think about, you know, what PyTorch is trying to do, it really is trying to be that sort of hardware-agnostic layer. And, you know, one of the sort of major milestones that we've come up with is on PyTorch 2.0, you know, AMD was qualified on sort of day one. And what that means is, you know, anybody who runs code on PyTorch right now, it will run on AMD, you know, sort of out of the box, because we've done the work there. And, frankly, it'll run on other hardware as well. But, you know, our goal is to make it, you know, sort of \"May the best chip win.\" And the way you do that is to make the software, you know, much more seamless. And, you know, it's PyTorch, but it's also JAX, it's also some of the tools that OpenAI is bringing in with Triton. There are lots of different tools and, you know, frameworks that people are bringing forward that are hardware-agnostic. There are a bunch of people who are also doing, you know, sort of build drone types of things. So I do think this is the wave of the future for AI software. - Are you building custom chips for any of these companies? - We have the capability of building custom chips and, you know, the way I think about it is the time to build custom chips is actually when you get very, a high volume applications, you know, going forward. So I do believe there will be custom chips, you know, over the next number of years. The other piece that's also interesting is you need all different types of engines for AI. So, you know, we spend a lot of time talking about big GPUs because that's what's needed for trading large language models. But you're also gonna see, you know, ASICS, for some, you know, let's call it more narrow applications. You're actually also gonna see AI in sort of client chips. So I'm pretty excited about that as well in terms of just, you know, how broad AI will be incorporated into, you know, chips across all the market segments. - I've got Kevin Scott, CTO of Microsoft here tomorrow. So I'll ask you this question so I can chase him down with it. If, say, Microsoft wanted to diversify Azure and put more AMD in there and be invisible to customers, is that possible right now? - Well, first of all, I love Kevin Scott, he's a great guy. And we have a tremendous partnership with Microsoft across, you know, both the cloud as well as the Windows environment. And I think you should ask him the question, but I think if you were to ask him, or if you were asked a bunch of other cloud manufacturers, they would say, \"It's absolutely possible.\" I mean, yes, it takes work. It takes work that we each have to put in, but it's much less work than you might have imagined because people are actually designing at these higher level, they're writing code at the higher level frameworks. And, you know, we believe that, you know, this is the wave of the future for AI programming. - Let me connect this to an end-user application just for a second. We're talking about things that are very much rising the cost curve. Right here, a lot of smart people doing a lot of work to develop for really high-end GPUs on the cutting edge process nodes. Everything's just getting more expensive. And you see how the consumer applications are expensive, $25 a month, $30 a seat for Microsoft Office with CoPilot. When do you come down the cost curve that brings those consumer prices down? - Yeah, no, it's a great, great question. I do believe that, you know, sort of the value that you get with Gen AI in terms of productivity will absolutely be proven out. So, yes, the cost of these infrastructures are high right now, but the productivity that you get on the other side is also exciting. I mean, you know, we're deploying AI internally within AMD and it's like such high-priority, because if I can get chips out faster, I mean, that's huge productivity. - Do you trust it? Do you have your people checking the work that AI is doing, or do you trust it? - Sure, sure. I mean, look, we're all experimenting, right? I mean, we're in the very early stages of building the tools and the infrastructure so that we can deploy. But the fact is, it saves us time. You know, whether we're designing chips, whether we're testing chips, whether we're validating chips, it saves us time, and time is money in our world. But back to your question about, \"When do you get to the other side of the curve?\" I think that's why it's so important to think about AI broadly and not just in the cloud, right? So if you think about the, you know, sort of how the ecosystem will look a few years from now, you would imagine a place where, yes, you have, you know, sort of the cloud infrastructures training these, you know, largest foundational models, but you're also gonna have a bunch of AI at the edge and, you know, whether it's in your PC or it's in your phone, you're gonna be able to do, you know, local AI. And there, it's cheaper, it is faster, and it is actually more private when you do that. And so that's this idea of AI everywhere and how it can really, you know, sort of enhance the way we're deploying. - That brings me to open source, and honestly, to the idea of how we will regulate this. So I even, you know, there's a White House meeting, everyone participates, great, everyone's very proud of each other. You think about how you will actually enforce AI regulation and it's, okay, you could probably tell AWS or Azure not to run certain work streams, right? You don't do these things, and that seems fine. Can you tell AMD to not let certain things happen on the chips for somebody running an open source model on Linux on their laptop? - Yeah, look, I think it is something that we all take, you know, very seriously. I think we want... The technology has so much upside in terms of, you know, what it can do from a productivity and a discovery standpoint. But, you know, there's also this, you know, sort of safety in AI. And I do think, that as large companies, we have a responsibility. You know, if you think about, you know, the two things around, you know, sort of data privacy as well as just overall ensuring that, you know, as these models are developed, that they're developed without, to the best of our ability, without too much bias. We're gonna make mistakes. I mean, you know, the industry, as a whole, will not be perfect here, but I think there is, you know, clarity around. It's important and that we need to do it together, and that there needs to be a public-private partnership you know, to make it happen. - I can't remember anyone's name, so it'll be a horrible politician. But let's pretend I'm a regulator, I'm gonna do it, and I say, \"Boy, I really don't want these kids using any model to develop chemical weapons, using... And I need to figure out where to land that enforcement. I can definitely tell Azure, \"Don't do that.\" But a kid with a AMD chip and a Dell laptop, running Linux, I have no mechanism of enforcement except to tell you to make the chip not do it. Would you accept that regulation? - I don't think it's a... There's a silver bullet. - [Nilay] Yeah. - It's not a, \"I can make the chip not do it.\" I can make the combination of the chip and the model at, you know, sort of have some safeguards in place. And we're absolutely willing to be at that table to help that happen. - So you would accept that kind of regulation that the chip will be constrained? - Yes, I would accept an opportunity for us to look at, you know, what are the safeguards that we would need to put in. - Yeah. I think this is gonna be one of the most complicate... I don't think we expect our chips to be limited in what we can do, and that it feels like this is a question we have to ask and answer. - Yeah, and let me say again, it's not the chip by itself, right? - Yeah. - Because in general, you know, chips have broad capability. It's the chips, plus the software and the models, particularly on the model side, you know, what you do in terms of safeguards. - We could start lining up for questions. I've just got a couple more for you. You're in the PS5, you're in the Xbox. There's a view of the world that says, \"Cloud gaming is the future of all things.\" That might be great for you 'cause you'll be in their data centers too. But do you see that shift underway? Is that for real or are we still doing console generations? - Gaming is everywhere. I mean, gaming is everywhere in every form factor. There's been this long conversation about, you know, \"Is this the end of console gaming?\" And, you know, I don't see it. I see PC gaming strong, I see console gaming strong. And I see cloud gaming, you know, also having leagues and, you know, they all need similar types of technology, but they obviously use it in different ways. (futuristic music) - Please introduce yourself. - Alan Lee, Analog Devices. One and a half years after the Xilinx acquisition, how do you see adaptive computing playing out in AI? - Yeah, so first of all, it's nice to see you, Alan. I think the... You know, first of all, the Xilinx acquisition was an acquisition we completed about 18 months ago. Fantastic acquisition, you know, brought a lot of IP into our, you know, sort of high-performance IP with adaptive computing IP. And I do see that, particularly on these AI engines. So engines that are optimized for, you know, sort of data flow architectures. That's one of the things that we were able to bring in as part of Xilinx. That's actually the IP that is now going into PCs, and so we see, you know, significant, you know, sort of IP, you know, usage there. And together, as we go forward, you know, I have this belief that there's no one computer that is, you know, the right one there. You actually need the right computing for the right application. So whether it's CPUs or GPUs or FPGAs or adaptive SoCs, you need all of those. And that's the ecosystem that we're bringing together. Thanks. - This tall gentleman over here. - Hi. Casey Newton from Platformer. I wanted to return to Nilay's question about regulation. Someday, sad to say, but somebody might try to acquire a bunch of your GPUs for the express purpose of doing harm, right? Training a large language model for that purpose. And so I wonder, what sort of regulations, if any, do you think government should place around who gets access to large numbers of GPUs, and what size training runs they're allowed to do? - Yeah, that's a good question. I don't think we know the answer to that, particularly, in terms of how to regulate. Our goal is, you know, again, within all of the export controls that are out there, because GPUs are export controlled, that we follow, you know, those regulations. There are, you know, sort of the biggest, and then there're the, you know, sort of the next level of GPUs that are there. I think the key is, you know, again, as I said, it's a combination of both chip and model development that really, you know, comes about. And, you know, we're active at those tables and talking about, you know, how to do those things. I think we want to ensure that, you know, we are very protective of the highest performing GPUs, but also, you know, it's an important market where lots of people want access. Thank you. - Hi, I'm Daniel from DiA. To return something you talked about earlier, because you are, everyone here is thinking about implementing AI in their internal workflows and it's just so interesting to hear about your thoughts because you have access to the chips and deep machine learning knowledge. Can you specify a bit, what are you using AI internally for in the chip making process and anything? Because this might point us in the right direction. - Yeah, thanks for the question. I think every business is looking at how to implement AI. So for us, for example, I look at it at, you know, there's the engineering functions and the non-engineering sort of sales, marketing, data analytics, all of those lead generation, those are all places where AI can be very useful. On the engineering side, we look at it in terms of, you know, how can we, you know, build chips faster? So they help us with design, they help us with test generation, they help us with manufacturing diagnostics. You know, back to Nilay's question, \"Do I trust it to build a chip with no humans involved?\" No, of course not. - Yeah. (chuckling) - I mean, we have lots of engineers. I think CoPilot functions in particular, are actually fairly easy to adopt. You know, pure Generative AI, we need to, you know, check and make sure that it works, but it's a learning process. And, you know, the key I would say is, there's lots of experimentation and fast cycles of learning are important. So, you know, we've actually have, you know, dedicated teams that are, you know, spending their time looking at how we bring AI into our company development processes as fast as possible. Thank you. - Hi, Jay Peters with The Verge. Apple seems to be making a much bigger push in how its devices, and particularly its M series chips are really good for triple A gaming. Are you worried about Apple on that front at all? - Well- - Yeah, they told me that this, I don't have my phone on me. But they told me the iPhone 15 Pro is the world's best game console. And that's why it's pro, it's a very confusing situation. - Yeah, I don't know about that. (Dr. Lisa) (Nilay laughing) I would say... Look, as I said earlier, gaming is such an important application when you think about entertainment and what we're doing with it. I mean, I always think about all competition, but from my standpoint, it's, \"How do we get,\" you know, it's not just the hardware, it's really, \"How do we get the gaming ecosystem?\" You know, people wanna be able to take their games wherever and play with their friends and on different platforms. Those are, you know, options that we have with the gaming ecosystem today. We're gonna continue to push the envelope on the highest performing, you know, PCs, and console chips, and I think we're gonna be pretty good. - Thank you. - Thank you. - All right, I have one more for you. If you listen to Decoder, you know, I love asking people about decisions. Chips CEOs have to make the longest range decisions of basically anybody I can think of. What's the longest term bet you're making right now? - We're definitely designing for the five plus year cycle. So when I think about for, you know, I talked to you today about MI300. We made, you know, some of those architectural decisions, you know, four or five years ago. And the thought process there was, \"Hey, where's the world going? What kinda computing do you need?\" Being very ambitious in, you know, our goals and what we were trying to do. So, yeah, we're pretty excited about what we're building for the next five years. I think we're- - What's a bet you're making right now? - We're betting on what the next big thing in AI is. - Okay, thank you, Lisa. - All right. - I did my best. (audience applauding) (upbeat music) I'd like to thank Dr. Lisa Su for talking to me at the Code Conference and especially to the Code audience for coming to the show. As always, I'd love to hear what you think of the show. You can email us at decoder@theverge.com, I read all the emails. Or you can hit me up directly on Threads, I'm @reckless1280. We also have a TikTok. Check it out, it's @decoderpodcast. It's a lot of fun.",
    "transcript_keywords": [
        "CHIPS",
        "sort",
        "AMD",
        "chip",
        "Lisa",
        "Yeah",
        "lot",
        "people",
        "models",
        "things",
        "large language model",
        "GPUs",
        "years",
        "market",
        "time",
        "model",
        "chip Lisa told",
        "software",
        "language model",
        "supply"
    ],
    "transcript_entity_values": [
        "Azure",
        "about 18 months ago",
        "Decoder",
        "Casey Newton",
        "CTO",
        "White House",
        "AMD",
        "Nilay",
        "PyTorch",
        "Gen AI",
        "Lisa Su",
        "the last few years",
        "25",
        "the end of the day",
        "a few years",
        "the next number of years",
        "adaptive computing IP",
        "The CHIPS and Science Act",
        "Generative AI",
        "seven nanometer",
        "PyTorch 2.0",
        "a few months",
        "VC",
        "the five plus year",
        "10, 20 years",
        "Ryzen",
        "One",
        "Triton",
        "TikTok",
        "Microsoft",
        "AI",
        "H100",
        "Huawei",
        "U.S.",
        "Xbox",
        "MI300",
        "this week",
        "MI300",
        "MI300",
        "GPU",
        "the next 10 years",
        "half an hour",
        "the Code Conference",
        "Pat Gelsinger",
        "this morning",
        "Nilay",
        "Samsung",
        "Xilinx",
        "China",
        "a few years from now",
        "first",
        "Kevin Scott",
        "Daniel",
        "IP",
        "hundreds of billions",
        "The Code Conference",
        "Linux",
        "Lamini",
        "a ton",
        "10",
        "last year",
        "Today",
        "PS5",
        "Dell",
        "four or five years ago",
        "10-year",
        "AI",
        "AMD",
        "PyTorch",
        "Ohio",
        "Alan",
        "the next five years",
        "TSMC",
        "Lisa",
        "these days",
        "second",
        "Gina Raimondo",
        "Lamini",
        "Threads",
        "the last 30 years",
        "Intel",
        "tomorrow",
        "The Verge",
        "the next two to four quarters",
        "Microsoft Office with CoPilot",
        "AWS",
        "one",
        "Nilay Patel",
        "LLaMA",
        "500",
        "Apple",
        "Jay Peters",
        "today",
        "Linux",
        "five years ago",
        "Alan Lee",
        "JAX",
        "ASICS",
        "the last 10",
        "a few years ago",
        "the last three or four years",
        "day one",
        "15",
        "Pat",
        "the United States",
        "two",
        "MI300",
        "all night",
        "Analog Devices",
        "Japan",
        "the coming days",
        "Nvidia",
        "100",
        "the last five or seven or eight years",
        "30",
        "These past few years",
        "CUDA"
    ],
    "transcript_entity_types": [
        "ORG",
        "DATE",
        "ORG",
        "PERSON",
        "ORG",
        "ORG",
        "ORG",
        "PERSON",
        "PERSON",
        "ORG",
        "PERSON",
        "DATE",
        "MONEY",
        "DATE",
        "DATE",
        "DATE",
        "ORG",
        "ORG",
        "ORG",
        "QUANTITY",
        "ORG",
        "DATE",
        "ORG",
        "DATE",
        "DATE",
        "PERSON",
        "CARDINAL",
        "PERSON",
        "ORG",
        "ORG",
        "GPE",
        "PRODUCT",
        "ORG",
        "GPE",
        "FAC",
        "PRODUCT",
        "DATE",
        "FAC",
        "WORK_OF_ART",
        "ORG",
        "DATE",
        "TIME",
        "EVENT",
        "PERSON",
        "TIME",
        "PRODUCT",
        "ORG",
        "PRODUCT",
        "GPE",
        "DATE",
        "ORDINAL",
        "PERSON",
        "PERSON",
        "ORG",
        "MONEY",
        "LAW",
        "GPE",
        "PERSON",
        "QUANTITY",
        "CARDINAL",
        "DATE",
        "DATE",
        "ORG",
        "ORG",
        "DATE",
        "DATE",
        "ORG",
        "PRODUCT",
        "ORG",
        "GPE",
        "PERSON",
        "DATE",
        "ORG",
        "PERSON",
        "DATE",
        "ORDINAL",
        "PERSON",
        "GPE",
        "PERSON",
        "DATE",
        "ORG",
        "DATE",
        "ORG",
        "DATE",
        "ORG",
        "ORG",
        "CARDINAL",
        "PERSON",
        "DATE",
        "CARDINAL",
        "ORG",
        "PERSON",
        "DATE",
        "ORG",
        "DATE",
        "PERSON",
        "ORG",
        "ORG",
        "DATE",
        "DATE",
        "DATE",
        "DATE",
        "CARDINAL",
        "PERSON",
        "GPE",
        "CARDINAL",
        "PERSON",
        "TIME",
        "ORG",
        "GPE",
        "DATE",
        "ORG",
        "CARDINAL",
        "DATE",
        "MONEY",
        "DATE",
        "ORG"
    ],
    "vector": [
        -0.06718273460865021,
        -0.05588570982217789,
        0.004022362641990185,
        -0.008908228017389774,
        0.09223569929599762,
        -0.06118068844079971,
        -0.059930942952632904,
        0.02339892089366913,
        0.035713884979486465,
        0.005803236737847328,
        -0.07692272961139679,
        0.020187988877296448,
        -0.02913074754178524,
        0.0038147317245602608,
        -0.05376375466585159,
        0.04149988293647766,
        0.09323843568563461,
        -0.05542773753404617,
        -0.0653681680560112,
        -0.022309446707367897,
        -0.018161041662096977,
        -0.05081025883555412,
        -0.010161849670112133,
        0.027291376143693924,
        -0.03715571016073227,
        0.02363428846001625,
        -0.004271866753697395,
        -0.08494041860103607,
        0.024313997477293015,
        -0.011014139279723167,
        -0.015659065917134285,
        0.03423997759819031,
        0.11444399505853653,
        0.042503297328948975,
        -0.010938825085759163,
        -0.03856613114476204,
        0.022832931950688362,
        -0.06847799569368362,
        -0.061474744230508804,
        -0.06648972630500793,
        0.03941933810710907,
        -0.07578040659427643,
        -0.02175823040306568,
        0.008318513631820679,
        0.06316786259412766,
        -0.04942529648542404,
        0.021446628496050835,
        -0.049533095210790634,
        0.017640739679336548,
        -0.040780115872621536,
        -0.04673023521900177,
        -0.059809453785419464,
        0.019183985888957977,
        -0.04630957543849945,
        -0.06309027969837189,
        0.03523586690425873,
        -0.008884452283382416,
        -0.06608599424362183,
        0.0709470883011818,
        0.004230548162013292,
        0.061361439526081085,
        -0.08165513724088669,
        -0.0044527421705424786,
        0.09336678683757782,
        0.04681045562028885,
        0.016206886619329453,
        -0.005740458145737648,
        0.019528253003954887,
        -0.08328661322593689,
        -0.019725171849131584,
        0.06583738327026367,
        -0.01884169690310955,
        0.030127916485071182,
        -0.03724602237343788,
        0.014490058645606041,
        0.039253346621990204,
        0.09218093007802963,
        -0.021470915526151657,
        0.08116289973258972,
        -0.08257195353507996,
        0.07535278052091599,
        -0.06644615530967712,
        0.05118609592318535,
        -0.003987922333180904,
        0.05196104571223259,
        -0.03934700787067413,
        0.05642681196331978,
        0.024994710460305214,
        0.039369650185108185,
        -0.058009032160043716,
        -0.09780042618513107,
        -0.008688805624842644,
        0.05170207470655441,
        -0.026924317702651024,
        0.05460372567176819,
        0.004326328169554472,
        -0.07191921770572662,
        -0.11025091260671616,
        -0.054233141243457794,
        0.046998172998428345,
        -0.027006441727280617,
        0.07882651686668396,
        0.013227216899394989,
        0.0029552362393587828,
        -0.06592521071434021,
        0.02261737920343876,
        0.003994857892394066,
        0.10314515978097916,
        0.0043157050386071205,
        0.0685775950551033,
        -0.0023524751886725426,
        0.11008794605731964,
        -0.024524347856640816,
        -0.02653399668633938,
        0.07466525584459305,
        0.016115689650177956,
        -0.0235120989382267,
        0.06306017935276031,
        0.11203628033399582,
        -0.0009807264432311058,
        -0.0582156740128994,
        0.017383381724357605,
        -0.06572505086660385,
        0.020444808527827263,
        0.031326547265052795,
        0.03400883078575134,
        -0.09122913330793381,
        6.253687251724172e-33,
        -0.04318567365407944,
        0.03641986846923828,
        -0.01901973783969879,
        -0.03904630243778229,
        0.03625016286969185,
        0.08215957134962082,
        0.0045273019932210445,
        0.0008299321634694934,
        0.004539287183433771,
        -0.0008911509648896754,
        -0.023636432364583015,
        0.031048351898789406,
        -0.003158740233629942,
        0.0759880468249321,
        0.0230571199208498,
        -0.08865407854318619,
        0.012490621767938137,
        0.02933894656598568,
        -0.03823506087064743,
        -0.033296260982751846,
        0.0633644387125969,
        -0.0036687105894088745,
        -0.027044255286455154,
        -0.015768976882100105,
        0.002040144056081772,
        -0.004593153018504381,
        -0.06983247399330139,
        -0.025762079283595085,
        0.09300968796014786,
        0.03600951284170151,
        -0.046407122164964676,
        -0.013143337331712246,
        -0.04419831931591034,
        -0.0064950077794492245,
        0.007831355556845665,
        0.07300566881895065,
        -0.044052183628082275,
        -0.04095454141497612,
        0.00974645372480154,
        0.06811206042766571,
        -0.06678859889507294,
        0.08834522217512131,
        -0.014442602172493935,
        -0.15608006715774536,
        -0.07161302119493484,
        0.031494565308094025,
        0.052242591977119446,
        -0.06694167107343674,
        -0.01602189615368843,
        0.005244329106062651,
        -0.08298970758914948,
        0.059763532131910324,
        -0.025157174095511436,
        -0.05154203996062279,
        0.07025370001792908,
        -0.0839855819940567,
        0.05905720591545105,
        -0.04066651314496994,
        0.07450482249259949,
        -0.02384329028427601,
        -0.049031149595975876,
        -0.029499035328626633,
        -0.10975464433431625,
        0.043430786579847336,
        -0.09058117866516113,
        0.1001688614487648,
        0.038591790944337845,
        0.054001569747924805,
        -0.06969034671783447,
        0.04262132570147514,
        -0.07361581921577454,
        0.044520486146211624,
        -0.02247179113328457,
        -0.02182675339281559,
        -0.0227412898093462,
        0.056111354380846024,
        -0.048429254442453384,
        -0.03775887191295624,
        -0.009386003017425537,
        0.008452771231532097,
        -0.017812618985772133,
        0.01714838668704033,
        0.0384560190141201,
        -0.037386488169431686,
        0.03549172356724739,
        0.008557315915822983,
        -0.013604506850242615,
        0.02124018408358097,
        -0.013342361897230148,
        -0.025871336460113525,
        -0.08865542709827423,
        -0.028684237971901894,
        0.06612126529216766,
        0.10766182839870453,
        -0.06724827736616135,
        -7.63643080748803e-33,
        -0.11863595992326736,
        0.01941731944680214,
        -0.011289029382169247,
        0.04172515124082565,
        0.004727290943264961,
        -0.043786779046058655,
        0.03611743450164795,
        -0.02715686894953251,
        0.06587020307779312,
        -0.07362265884876251,
        -0.011389176361262798,
        0.011977152898907661,
        0.03483852371573448,
        0.05439908802509308,
        0.007380354683846235,
        -0.019624264910817146,
        0.012145576067268848,
        -0.07696820795536041,
        -0.03916046395897865,
        -0.05402477830648422,
        0.02502078376710415,
        0.10254692286252975,
        -0.07884880155324936,
        0.005557464901357889,
        0.023846514523029327,
        0.097805455327034,
        0.05297712981700897,
        0.011888794600963593,
        0.037727002054452896,
        -0.015580220147967339,
        -0.07371633499860764,
        0.009562579914927483,
        -0.06984459608793259,
        0.06913399696350098,
        0.0667421892285347,
        0.0888795256614685,
        0.01710396260023117,
        -0.03410400450229645,
        -0.06443705409765244,
        -0.002146558603271842,
        0.06952305138111115,
        0.04607526957988739,
        -0.0006530467653647065,
        0.03602287545800209,
        -0.03843287006020546,
        0.010792920365929604,
        -0.020705994218587875,
        -0.04114289954304695,
        -0.005908029619604349,
        -0.03229725360870361,
        -0.07658587396144867,
        -0.018323678523302078,
        0.02752804011106491,
        0.03111928515136242,
        -0.05483846366405487,
        -0.04325730353593826,
        -0.0014022557297721505,
        0.10835212469100952,
        0.016073234379291534,
        -0.016932908445596695,
        0.05163054168224335,
        0.01977628469467163,
        -0.05165084823966026,
        -0.047395091503858566,
        -0.003605236066505313,
        -0.003884699195623398,
        0.06572473794221878,
        0.02627396211028099,
        -0.0411924384534359,
        -0.049806565046310425,
        0.038483940064907074,
        -0.035523876547813416,
        -0.1071358248591423,
        -0.07147679477930069,
        -0.0988454595208168,
        0.05321156606078148,
        -0.04217302054166794,
        -0.05257084593176842,
        -0.0392572320997715,
        0.03239371255040169,
        -0.0044052558951079845,
        -0.058365948498249054,
        0.04994139075279236,
        0.034042611718177795,
        0.07791823893785477,
        -0.0006322510889731348,
        0.0719320997595787,
        -0.05051572620868683,
        -0.0015203743241727352,
        0.005813347641378641,
        -0.09983731061220169,
        -0.017052965238690376,
        -0.06079202517867088,
        0.05436430126428604,
        -0.01394590362906456,
        -6.321874224113344e-08,
        -0.0005579782300628722,
        -0.07439526915550232,
        0.007622958160936832,
        0.033761925995349884,
        0.06934240460395813,
        -0.06129138544201851,
        0.00605460861697793,
        -0.04910256713628769,
        -0.02474784292280674,
        0.028202980756759644,
        0.10330197215080261,
        -0.08347232639789581,
        -0.06621494889259338,
        0.02842460758984089,
        0.0731416568160057,
        0.045201219618320465,
        -0.06684540212154388,
        0.029057539999485016,
        0.014279903843998909,
        -0.05728498473763466,
        -0.0041628628969192505,
        0.027745796367526054,
        0.046861592680215836,
        -0.0805235430598259,
        -0.043576985597610474,
        0.010151105932891369,
        0.009804983623325825,
        -0.02372187189757824,
        0.029739174991846085,
        0.038617704063653946,
        -0.05512826517224312,
        0.03358584642410278,
        0.05755634233355522,
        -0.041398148983716965,
        0.005227881949394941,
        -0.04732576757669449,
        0.08705100417137146,
        0.032052941620349884,
        0.11976668983697891,
        0.025248724967241287,
        0.0029075988568365574,
        0.006978748366236687,
        0.020808152854442596,
        0.031551577150821686,
        0.03266012668609619,
        -0.008302164264023304,
        -0.04278260096907616,
        -0.07414716482162476,
        0.017375798895955086,
        0.005909644532948732,
        -0.0030487817712128162,
        -0.02910342440009117,
        0.04409298300743103,
        0.06259441375732422,
        0.07359123229980469,
        -0.05280204862356186,
        -0.04516460373997688,
        -0.003176991594955325,
        -0.0555805079638958,
        0.054462678730487823,
        0.057811178267002106,
        -0.08063927292823792,
        0.02323910966515541,
        -0.0272505059838295
    ]
}